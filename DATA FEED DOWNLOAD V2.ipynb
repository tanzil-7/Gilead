{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ab0721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanzil.baraskar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\tanzil.baraskar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "c:\\Users\\tanzil.baraskar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "import csv\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import paramiko\n",
    "import re\n",
    "import math, calendar, datetime as dt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ba9578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login request with your identity provider. A browser window should have opened for you to complete the login. If you can't see it, check existing browser windows, or your OS settings. Press CTRL+C to abort and try again...\n",
      "Going to open: https://login.microsoftonline.com/d026e4c1-5892-497a-b9da-ee493c9f0364/saml2?SAMLRequest=pVPBbuIwFPyVyHtO4pi0EAuoaBFaJJZFEPbQS%2BXGL2Dh2Fnbaejf1wkgdQ%2FtZU%2B2nmc8897Y44dzJYM3MFZoNUFJhFEAqtBcqMME7fNFOEKBdUxxJrWCCXoHix6mY8sqWdNZ445qC38bsC7wFylLu4MJaoyimllhqWIVWOoKupv9WlESYcqsBeO8HLpSuBVe6%2BhcTeO4bduoHUTaHGKCMY5xFntUB%2FmBPknU32vURjtdaHmjnH1PX0gkMU47CY%2FwCpsr8VGoywi%2BU3m9gCz9meebcPN7l6NgduvuSSvbVGB2YN5EAfvt6mLAegcnocBaHV7Xl4opdgD%2BYqDWnqwOkVW6LSU7QaGruulLfheXwGOpD8LPbjmfoPokuD7zdbsapnqbnNrMPu4d44OcrZLF2S1Sty5Xz8ccyw3b6wIFf25Jky7ppbUNLFWXr%2FMlTO7CBIdkmBNMyYAmJEpG2TMK5j5foZjrmbcmeh9RJQqjrS6dVtJ307vkmNxDWiTh3SgjYZoNWfiacRYCpNmgyEo8uE%2FjLkWCLi%2BJ9kbM9H%2FnM44%2F33Z9pGuf23K%2B0VIU78FCm4q5r2NNoqSvCB6WPZRCxYSccW68Ex%2BvlLp9MsCc%2FwvONIDi6UX1398w%2FQA%3D&RelayState=ver%3A1-hint%3A1909228936499594-ETMsDgAAAZonVtA9ABRBRVMvQ0JDL1BLQ1M1UGFkZGluZwEAABAAEF%2ByjHpKvcxtUGDqe0eM340AAACwkhMLqI9SyMk1q0Qi%2FIn6QpJjLYvY7CHbyyT5OkNoSfelbQ88KhzJ1%2BXS3qEjzrL8AWWL4G3HLp26%2F8zCppg9Xsj1aun3AfFb8HsVJJZ1bM9w2FVFOwh7raCPnf0Z20JSB2E4CrvHUmE%2FUPd6HpSn8zCGqqq%2B81QS%2Bj%2BBZJrFWTZN3fU2hWbKpx23KOfpPkKF4628pbC6c1FhcwskJYxZBzD4OVWr214xQDmmxTOElSUAFFArIQbfHfpoW%2FSopjFUlz%2FDLZ2S to authenticate...\n"
     ]
    }
   ],
   "source": [
    "account = 'KINESSO-KINESSO_MANAGED_REPORTING'\n",
    "user = 'tanzil.baraskar@umj3.com'\n",
    "authenticator = 'externalbrowser'\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=user,\n",
    "    account=account,\n",
    "    authenticator=authenticator,\n",
    "    warehouse = 'WH_CLIENTTEAM_GILEAD_XS',\n",
    "    database = 'GILEAD',\n",
    "    schema = 'CUSTOM_OUTPUTS'\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c1872",
   "metadata": {},
   "source": [
    "# SEARCH KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a5375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = r\"C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\\SEARCH KEY\"\n",
    "YEAR = 2025\n",
    "MONTH = 9\n",
    "TABLE_A = \"GILEAD.CUSTOM_OUTPUTS.DATAFEED_SEARCH_KEY\"\n",
    "TABLE_B = \"GILEAD.CUSTOM_OUTPUTS.GILEAD_LOOKUP_SEARCH_BRAND_NAME\"\n",
    "\n",
    "# Columns in the exact order/names you want in the CSVs\n",
    "DESIRED_HEADERS = [\n",
    "    \"Franchise\",\n",
    "    \"Subsidiary\",\n",
    "    \"Brand\",\n",
    "    \"Brand Indication/Campaign\",\n",
    "    \"Row Type\",\n",
    "    \"Status\",\n",
    "    \"Sync Errors\",\n",
    "    \"Date\",\n",
    "    \"Week Sort\",\n",
    "    \"From\",\n",
    "    \"To\",\n",
    "    \"Engine\",\n",
    "    \"Account\",\n",
    "    \"Campaign\",\n",
    "    \"Campaign Type\",\n",
    "    \"Ad group\",\n",
    "    \"Keyword\",\n",
    "    \"Match type\",\n",
    "    \"Media Impressions\",\n",
    "    \"Media Clicks\",\n",
    "    \"Click Through Rate\",\n",
    "    \"Media Cost (USD)\",\n",
    "    \"Cost Per Click\",\n",
    "    \"Keyword landing page\",\n",
    "    \"GMID\",\n",
    "]\n",
    "\n",
    "\n",
    "def safe_token(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    s = s.strip().replace(\" \", \"_\")\n",
    "    s = re.sub(r\"[^\\w\\-.]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"NA\"\n",
    "\n",
    "def title_case_str(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    return s.strip().lower().title()\n",
    "\n",
    "def derive_strategy(account_val, campaign_val) -> str:\n",
    "    a = \"\" if account_val is None or (isinstance(account_val, float) and pd.isna(account_val)) else str(account_val).lower()\n",
    "    c = \"\" if campaign_val is None or (isinstance(campaign_val, float) and pd.isna(campaign_val)) else str(campaign_val).lower()\n",
    "    if \"dtc\" in a or \"_dtc_\" in c: return \"DTC\"\n",
    "    if \"dtp\" in a or \"_dtp_\" in c: return \"DTP\"\n",
    "    if \"hcp\" in a or \"_hcp_\" in c: return \"HCP\"\n",
    "    return \"NA\"\n",
    "\n",
    "def uniq_join(vals) -> str:\n",
    "    if vals is None:\n",
    "        return \"\"\n",
    "    s = pd.Series(list(vals))\n",
    "    s = s.dropna().astype(str).str.strip()\n",
    "    if s.empty:\n",
    "        return \"\"\n",
    "    return \"|\".join(sorted(set(s)))\n",
    "\n",
    "def write_group_csv(out_path: str, df_group: pd.DataFrame) -> int:\n",
    "    \"\"\"Write CSV with desired headers and return row count.\"\"\"\n",
    "    present_headers = [c for c in DESIRED_HEADERS if c in df_group.columns]\n",
    "    g_out = df_group[present_headers].copy()\n",
    "    g_out = (\n",
    "        g_out.assign(__DATE_PARSED__=df_group[\"__DATE_PARSED__\"])\n",
    "             .sort_values(by=\"__DATE_PARSED__\", na_position=\"last\")\n",
    "             .drop(columns=\"__DATE_PARSED__\")\n",
    "    )\n",
    "    g_out.to_csv(out_path, index=False)\n",
    "    return len(g_out)\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  a.\"Franchise\",\n",
    "  a.\"Subsidiary\",\n",
    "  a.\"Brand\",\n",
    "  a.\"Brand Indication/Campaign\",\n",
    "  a.\"Row Type\",\n",
    "  a.\"Status\",\n",
    "  a.\"Sync Errors\",\n",
    "  a.\"Date\",\n",
    "  a.\"Week Sort\",\n",
    "  a.\"From\",\n",
    "  a.\"To\",\n",
    "  a.\"Engine\",\n",
    "  a.\"Account\",\n",
    "  a.\"Campaign\",\n",
    "  a.\"Campaign Type\",\n",
    "  a.\"Ad group\",\n",
    "  a.\"Keyword\",\n",
    "  a.\"Match type\",\n",
    "  a.\"Media Impressions\",\n",
    "  a.\"Media Clicks\",\n",
    "  a.\"Click Through Rate\",\n",
    "  a.\"Media Cost (USD)\",\n",
    "  a.\"Cost Per Click\",\n",
    "  a.\"Keyword landing page\",\n",
    "  a.GMID,\n",
    "  b.TA,\n",
    "  b.BRAND_FILE_NAME\n",
    "FROM {TABLE_A} a\n",
    "LEFT JOIN {TABLE_B} b\n",
    "  ON a.\"Account\" = b.ACCOUNT   -- change to b.\"Account\" if lookup column is quoted/mixed-case\n",
    "WHERE EXTRACT(YEAR FROM a.\"Date\") = {YEAR}\n",
    "  AND EXTRACT(MONTH FROM a.\"Date\") = {MONTH}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main(conn):\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No rows for {MONTH}/{YEAR}.\")\n",
    "        return\n",
    "\n",
    "    dt = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    if dt.isna().all():\n",
    "        raise ValueError(\"All 'Date' values are NaT after parsing; check source data.\")\n",
    "    df[\"__DATE_PARSED__\"] = dt\n",
    "    df[\"__ACCOUNT__\"]  = df[\"Account\"]\n",
    "    df[\"__CAMPAIGN__\"] = df[\"Campaign\"]\n",
    "\n",
    "    df[\"__MONTH_YEAR__\"] = df[\"__DATE_PARSED__\"].dt.strftime(\"%B_%Y\")\n",
    "    folder_month_label = f\"{df['__DATE_PARSED__'].dt.strftime('%B').iloc[0]} {int(df['__DATE_PARSED__'].dt.year.iloc[0])}\"\n",
    "    df[\"__CHANNEL__\"] = \"SearchKey\"\n",
    "\n",
    "    if \"Campaign Type\" in df.columns:\n",
    "        COL_CTYPE = \"Campaign Type\"\n",
    "    elif \"Brand Type\" in df.columns:\n",
    "        COL_CTYPE = \"Brand Type\"\n",
    "    else:\n",
    "        COL_CTYPE = \"__CTYPE__\"\n",
    "        df[COL_CTYPE] = \"NA\"\n",
    "\n",
    "    df[\"__STRATEGY__\"] = df.apply(lambda r: derive_strategy(r[\"__ACCOUNT__\"], r[\"__CAMPAIGN__\"]), axis=1)\n",
    "\n",
    "    df[\"TA\"] = df[\"TA\"].fillna(\"\") if \"TA\" in df.columns else \"\"\n",
    "    brand_file_col = \"BRAND_FILE_NAME\" if \"BRAND_FILE_NAME\" in df.columns else None\n",
    "    has_brand_file = brand_file_col is not None\n",
    "\n",
    "    # Output folder\n",
    "    out_base = os.path.join(BASE_DIR, f\"{folder_month_label} Data Feed Delivery\", \"SearchKey\")\n",
    "    os.makedirs(out_base, exist_ok=True)\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    def add_to_summary(fname, fpath, subset):\n",
    "        entry = summary.setdefault(fname, {\n",
    "            \"Full Path\": fpath,\n",
    "            \"Rows\": 0,\n",
    "            \"Brand\": set(),\n",
    "            \"BIC\": set(),\n",
    "            \"CType\": set(),\n",
    "            \"Account\": set(),\n",
    "            \"TA\": set(),\n",
    "            \"Strategy\": set(),\n",
    "            \"Channel\": set(),\n",
    "            \"Month_Year\": set(),\n",
    "            \"LookupBrandFile\": set(),\n",
    "        })\n",
    "        entry[\"Rows\"] += len(subset)\n",
    "        for col, key in [\n",
    "            (\"Brand\", \"Brand\"),\n",
    "            (\"Brand Indication/Campaign\", \"BIC\"),\n",
    "            (COL_CTYPE, \"CType\"),\n",
    "            (\"Account\", \"Account\"),\n",
    "            (\"TA\", \"TA\"),\n",
    "            (\"__STRATEGY__\", \"Strategy\"),\n",
    "            (\"__CHANNEL__\", \"Channel\"),\n",
    "            (\"__MONTH_YEAR__\", \"Month_Year\"),\n",
    "        ]:\n",
    "            if col in subset.columns:\n",
    "                entry[key].update(subset[col].dropna().astype(str).str.strip().tolist())\n",
    "        if has_brand_file and brand_file_col in subset.columns:\n",
    "            entry[\"LookupBrandFile\"].update(subset[brand_file_col].dropna().astype(str).str.strip().tolist())\n",
    "\n",
    "    files_written = 0\n",
    "    rows_written = 0\n",
    "\n",
    "    # -------- Pass 1: BRANDED (group WITHOUT brand_file) --------\n",
    "    branded_mask = df[COL_CTYPE].astype(str).str.upper() == \"BRANDED\"\n",
    "    df_b = df[branded_mask].copy()\n",
    "    if not df_b.empty:\n",
    "        group_cols_branded = [\"TA\", \"__STRATEGY__\", \"__CHANNEL__\", COL_CTYPE, \"__MONTH_YEAR__\"]\n",
    "        for keys, g in df_b.groupby(group_cols_branded, dropna=False):\n",
    "            ta, strat, channel, ctype_val, month_year = keys\n",
    "            ctype_tc = title_case_str(ctype_val)\n",
    "            parts = [safe_token(ta), safe_token(strat), safe_token(channel), ctype_tc or \"NA\", safe_token(month_year)]\n",
    "            filename = \"_\".join([p for p in parts if p]) + \".csv\"\n",
    "            out_path = os.path.join(out_base, filename)\n",
    "\n",
    "            n = write_group_csv(out_path, g)\n",
    "            files_written += 1\n",
    "            rows_written += n\n",
    "            print(f\"Wrote (BRANDED): {out_path}  (rows: {n})\")\n",
    "\n",
    "            add_to_summary(filename, out_path, g)\n",
    "\n",
    "    # -------- Pass 2: UNBRANDED (group WITH brand_file) --------\n",
    "    unbranded_mask = df[COL_CTYPE].astype(str).str.upper() == \"UNBRANDED\"\n",
    "    df_u = df[unbranded_mask].copy()\n",
    "    if not df_u.empty:\n",
    "        if not has_brand_file:\n",
    "            # if lookup absent, still create a placeholder col to keep behavior stable\n",
    "            df_u[brand_file_col] = \"\"\n",
    "        group_cols_unbranded = [\"TA\", \"__STRATEGY__\", \"__CHANNEL__\", COL_CTYPE, \"__MONTH_YEAR__\", brand_file_col]\n",
    "        for keys, g in df_u.groupby(group_cols_unbranded, dropna=False):\n",
    "            ta, strat, channel, ctype_val, month_year, brand_file = keys\n",
    "            ctype_tc = title_case_str(ctype_val)\n",
    "            parts = [\n",
    "                safe_token(ta),\n",
    "                safe_token(strat),\n",
    "                safe_token(channel),\n",
    "                ctype_tc or \"NA\",\n",
    "                safe_token(brand_file),\n",
    "                safe_token(month_year),\n",
    "            ]\n",
    "            filename = \"_\".join([p for p in parts if p]) + \".csv\"\n",
    "            out_path = os.path.join(out_base, filename)\n",
    "\n",
    "            n = write_group_csv(out_path, g)\n",
    "            files_written += 1\n",
    "            rows_written += n\n",
    "            print(f\"Wrote (UNBRANDED): {out_path}  (rows: {n})\")\n",
    "\n",
    "            add_to_summary(filename, out_path, g)\n",
    "\n",
    "    # -------- Pass 3: OTHER ctypes (group WITHOUT brand_file) --------\n",
    "    others_mask = ~(branded_mask | unbranded_mask)\n",
    "    df_o = df[others_mask].copy()\n",
    "    if not df_o.empty:\n",
    "        group_cols_other = [\"TA\", \"__STRATEGY__\", \"__CHANNEL__\", COL_CTYPE, \"__MONTH_YEAR__\"]\n",
    "        for keys, g in df_o.groupby(group_cols_other, dropna=False):\n",
    "            ta, strat, channel, ctype_val, month_year = keys\n",
    "            ctype_tc = title_case_str(ctype_val)\n",
    "            parts = [safe_token(ta), safe_token(strat), safe_token(channel), ctype_tc or \"NA\", safe_token(month_year)]\n",
    "            filename = \"_\".join([p for p in parts if p]) + \".csv\"\n",
    "            out_path = os.path.join(out_base, filename)\n",
    "\n",
    "            n = write_group_csv(out_path, g)\n",
    "            files_written += 1\n",
    "            rows_written += n\n",
    "            print(f\"Wrote (OTHER): {out_path}  (rows: {n})\")\n",
    "\n",
    "            add_to_summary(filename, out_path, g)\n",
    "\n",
    "    print(f\"\\nDone. Files: {files_written}  Total rows written: {rows_written}\")\n",
    "\n",
    "    # -------- Write aggregated summary (one row per file) --------\n",
    "    if summary:\n",
    "        rows = []\n",
    "        for fname, d in summary.items():\n",
    "            rows.append({\n",
    "                \"File Name\": fname,\n",
    "                \"Full Path\": d[\"Full Path\"],\n",
    "                \"Rows\": d[\"Rows\"],\n",
    "                \"Brand (unique)\": uniq_join(d[\"Brand\"]),\n",
    "                \"Brand Indication/Campaign (unique)\": uniq_join(d[\"BIC\"]),\n",
    "                \"Campaign Type (unique)\": uniq_join(d[\"CType\"]),\n",
    "                \"Account (unique)\": uniq_join(d[\"Account\"]),\n",
    "                \"TA\": uniq_join(d[\"TA\"]),\n",
    "                \"Strategy\": uniq_join(d[\"Strategy\"]),\n",
    "                \"Channel\": uniq_join(d[\"Channel\"]),\n",
    "                \"Month_Year\": uniq_join(d[\"Month_Year\"]),\n",
    "                \"Lookup Brand File Name\": uniq_join(d[\"LookupBrandFile\"]),\n",
    "            })\n",
    "        summary_df = pd.DataFrame(rows)\n",
    "        summary_name = f\"SearchKey_File_Summary_{df['__DATE_PARSED__'].dt.strftime('%B_%Y').iloc[0]}.csv\"\n",
    "        summary_path = os.path.join(out_base, summary_name)\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        print(f\"Summary written: {summary_path}\")\n",
    "    else:\n",
    "        print(\"No files written; skipping summary.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        conn  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"Please create a Snowflake connection named `conn` before running this script.\")\n",
    "    main(conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448a1c0",
   "metadata": {},
   "source": [
    "# SEARCH GEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r\"C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\\SEARCH GEO\"\n",
    "YEAR = 2025\n",
    "MONTH = 8\n",
    "TABLE_A = \"GILEAD.CUSTOM_OUTPUTS.DATAFEED_SEARCH_GEO\"\n",
    "TABLE_B = \"GILEAD.CUSTOM_OUTPUTS.GILEAD_LOOKUP_SEARCH_BRAND_NAME\"\n",
    "\n",
    "# The exact headers (and order) you want in the CSVs (GEO schema)\n",
    "DESIRED_HEADERS = [\n",
    "    \"Franchise\",\n",
    "    \"Subsidiary\",\n",
    "    \"Brand\",\n",
    "    \"Brand Indication/Campaign\",\n",
    "    \"Row Type\",\n",
    "    \"Status\",\n",
    "    \"Sync Errors\",\n",
    "    \"Date\",\n",
    "    \"Week Sort\",\n",
    "    \"From\",\n",
    "    \"To\",\n",
    "    \"Engine\",\n",
    "    \"Account\",\n",
    "    \"Campaign\",\n",
    "    \"Campaign Type\",\n",
    "    \"Country\",\n",
    "    \"Country ID\",\n",
    "    \"DMA\",\n",
    "    \"DMA Code\",\n",
    "    \"State / Region\",\n",
    "    \"State / Region ID\",\n",
    "    \"City\",\n",
    "    \"City ID\",\n",
    "    \"Media Impressions\",\n",
    "    \"Media Clicks\",\n",
    "    \"Click Through Rate\",\n",
    "    \"Media Cost (USD)\",\n",
    "    \"Cost Per Click\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "def safe_token(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    s = s.strip().replace(\" \", \"_\")\n",
    "    s = re.sub(r\"[^\\w\\-.]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"NA\"\n",
    "\n",
    "def title_case_str(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    return s.strip().lower().title()\n",
    "\n",
    "def derive_strategy(account_val, campaign_val) -> str:\n",
    "    a = \"\" if account_val is None or (isinstance(account_val, float) and pd.isna(account_val)) else str(account_val).lower()\n",
    "    c = \"\" if campaign_val is None or (isinstance(campaign_val, float) and pd.isna(campaign_val)) else str(campaign_val).lower()\n",
    "    if \"dtc\" in a or \"_dtc_\" in c: return \"DTC\"\n",
    "    if \"dtp\" in a or \"_dtp_\" in c: return \"DTP\"\n",
    "    if \"hcp\" in a or \"_hcp_\" in c: return \"HCP\"\n",
    "    return \"NA\"\n",
    "\n",
    "def uniq_join(vals) -> str:\n",
    "    if vals is None:\n",
    "        return \"\"\n",
    "    s = pd.Series(list(vals))\n",
    "    s = s.dropna().astype(str).str.strip()\n",
    "    if s.empty:\n",
    "        return \"\"\n",
    "    # match Search Key: dedup + SORTED\n",
    "    return \"|\".join(sorted(set(s)))\n",
    "\n",
    "def write_group_csv(out_path: str, df_group: pd.DataFrame) -> int:\n",
    "    \"\"\"Write CSV with desired headers and return row count.\"\"\"\n",
    "    present_headers = [c for c in DESIRED_HEADERS if c in df_group.columns]\n",
    "    g_out = df_group[present_headers].copy()\n",
    "    g_out = (\n",
    "        g_out.assign(__DATE_PARSED__=df_group[\"__DATE_PARSED__\"])\n",
    "             .sort_values(by=\"__DATE_PARSED__\", na_position=\"last\")\n",
    "             .drop(columns=\"__DATE_PARSED__\")\n",
    "    )\n",
    "    g_out.to_csv(out_path, index=False)\n",
    "    return len(g_out)\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  a.\"Franchise\",\n",
    "  a.\"Subsidiary\",\n",
    "  a.\"Brand\",\n",
    "  a.\"Brand Indication/Campaign\",\n",
    "  a.\"Row Type\",\n",
    "  a.\"Status\",\n",
    "  a.\"Sync Errors\",\n",
    "  a.\"Date\",\n",
    "  a.\"Week Sort\",\n",
    "  a.\"From\",\n",
    "  a.\"To\",\n",
    "  a.\"Engine\",\n",
    "  a.\"Account\",\n",
    "  a.\"Campaign\",\n",
    "  a.\"Campaign Type\",\n",
    "  a.\"Country\",\n",
    "  a.\"Country ID\",\n",
    "  a.DMA,\n",
    "  a.\"DMA Code\",\n",
    "  a.\"State / Region\",\n",
    "  a.\"State / Region ID\",\n",
    "  a.\"City\",\n",
    "  a.\"City ID\",\n",
    "  a.\"Media Impressions\",\n",
    "  a.\"Media Clicks\",\n",
    "  a.\"Click Through Rate\",\n",
    "  a.\"Media Cost (USD)\",\n",
    "  a.\"Cost Per Click\",\n",
    "  b.TA,\n",
    "  b.BRAND_FILE_NAME\n",
    "FROM {TABLE_A} a\n",
    "LEFT JOIN {TABLE_B} b\n",
    "  ON a.\"Account\" = b.ACCOUNT   -- change to b.\"Account\" if lookup column is quoted/mixed-case\n",
    "WHERE EXTRACT(YEAR FROM a.\"Date\") = {YEAR}\n",
    "  AND EXTRACT(MONTH FROM a.\"Date\") = {MONTH}\n",
    "\"\"\"\n",
    "\n",
    "def main(conn):\n",
    "    df = pd.read_sql(query, conn)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No rows for {MONTH}/{YEAR}.\")\n",
    "        return\n",
    "\n",
    "    # ---- INTERNAL HELPER COLUMNS ----\n",
    "    dt = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    if dt.isna().all():\n",
    "        raise ValueError(\"All 'Date' values are NaT after parsing; check source data.\")\n",
    "    df[\"__DATE_PARSED__\"] = dt\n",
    "    df[\"__ACCOUNT__\"]  = df[\"Account\"]\n",
    "    df[\"__CAMPAIGN__\"] = df[\"Campaign\"]\n",
    "\n",
    "    df[\"__MONTH_YEAR__\"] = df[\"__DATE_PARSED__\"].dt.strftime(\"%B_%Y\")\n",
    "    folder_month_label = f\"{df['__DATE_PARSED__'].dt.strftime('%B').iloc[0]} {int(df['__DATE_PARSED__'].dt.year.iloc[0])}\"\n",
    "    df[\"__CHANNEL__\"] = \"SearchGeo\"\n",
    "\n",
    "    # Campaign type column selection\n",
    "    if \"Campaign Type\" in df.columns:\n",
    "        COL_CTYPE = \"Campaign Type\"\n",
    "    elif \"Brand Type\" in df.columns:\n",
    "        COL_CTYPE = \"Brand Type\"\n",
    "    else:\n",
    "        COL_CTYPE = \"__CTYPE__\"\n",
    "        df[COL_CTYPE] = \"NA\"\n",
    "\n",
    "    # Strategy\n",
    "    df[\"__STRATEGY__\"] = df.apply(lambda r: derive_strategy(r[\"__ACCOUNT__\"], r[\"__CAMPAIGN__\"]), axis=1)\n",
    "\n",
    "    # Lookup columns\n",
    "    df[\"TA\"] = df[\"TA\"].fillna(\"\") if \"TA\" in df.columns else \"\"\n",
    "    brand_file_col = \"BRAND_FILE_NAME\" if \"BRAND_FILE_NAME\" in df.columns else None\n",
    "    has_brand_file = brand_file_col is not None\n",
    "\n",
    "    # Output folder\n",
    "    out_base = os.path.join(BASE_DIR, f\"{folder_month_label} Data Feed Delivery\", \"SearchGeo\")\n",
    "    os.makedirs(out_base, exist_ok=True)\n",
    "\n",
    "    # -------- Summary aggregator keyed by filename (same as Search Key) --------\n",
    "    summary = {}\n",
    "\n",
    "    def add_to_summary(fname, fpath, subset):\n",
    "        entry = summary.setdefault(fname, {\n",
    "            \"Full Path\": fpath,\n",
    "            \"Rows\": 0,\n",
    "            \"Brand\": set(),\n",
    "            \"BIC\": set(),\n",
    "            \"CType\": set(),\n",
    "            \"Account\": set(),\n",
    "            \"TA\": set(),\n",
    "            \"Strategy\": set(),\n",
    "            \"Channel\": set(),\n",
    "            \"Month_Year\": set(),\n",
    "            \"LookupBrandFile\": set(),\n",
    "        })\n",
    "        entry[\"Rows\"] += len(subset)\n",
    "        for col, key in [\n",
    "            (\"Brand\", \"Brand\"),\n",
    "            (\"Brand Indication/Campaign\", \"BIC\"),\n",
    "            (COL_CTYPE, \"CType\"),\n",
    "            (\"Account\", \"Account\"),\n",
    "            (\"TA\", \"TA\"),\n",
    "            (\"__STRATEGY__\", \"Strategy\"),\n",
    "            (\"__CHANNEL__\", \"Channel\"),\n",
    "            (\"__MONTH_YEAR__\", \"Month_Year\"),\n",
    "        ]:\n",
    "            if col in subset.columns:\n",
    "                entry[key].update(subset[col].dropna().astype(str).str.strip().tolist())\n",
    "        if has_brand_file and brand_file_col in subset.columns:\n",
    "            entry[\"LookupBrandFile\"].update(subset[brand_file_col].dropna().astype(str).str.strip().tolist())\n",
    "\n",
    "    files_written = 0\n",
    "    rows_written = 0\n",
    "\n",
    "    # -------- Pass 1: BRANDED (group WITHOUT brand_file) --------\n",
    "    branded_mask = df[COL_CTYPE].astype(str).str.upper() == \"BRANDED\"\n",
    "    df_b = df[branded_mask].copy()\n",
    "    if not df_b.empty:\n",
    "        group_cols_branded = [\"TA\", \"__STRATEGY__\", \"__CHANNEL__\", COL_CTYPE, \"__MONTH_YEAR__\"]\n",
    "        for keys, g in df_b.groupby(group_cols_branded, dropna=False):\n",
    "            ta, strat, channel, ctype_val, month_year = keys\n",
    "            ctype_tc = title_case_str(ctype_val)\n",
    "            parts = [safe_token(ta), safe_token(strat), safe_token(channel), ctype_tc or \"NA\", safe_token(month_year)]\n",
    "            filename = \"_\".join([p for p in parts if p]) + \".csv\"\n",
    "            out_path = os.path.join(out_base, filename)\n",
    "\n",
    "            n = write_group_csv(out_path, g)\n",
    "            files_written += 1\n",
    "            rows_written += n\n",
    "            print(f\"Wrote (BRANDED): {out_path}  (rows: {n})\")\n",
    "\n",
    "            add_to_summary(filename, out_path, g)\n",
    "\n",
    "    # -------- Pass 2: UNBRANDED (group WITH brand_file) --------\n",
    "    unbranded_mask = df[COL_CTYPE].astype(str).str.upper() == \"UNBRANDED\"\n",
    "    df_u = df[unbranded_mask].copy()\n",
    "    if not df_u.empty:\n",
    "        if not has_brand_file:\n",
    "            # if lookup absent, still create a placeholder col to keep behavior stable\n",
    "            df_u[brand_file_col] = \"\"\n",
    "        group_cols_unbranded = [\"TA\", \"__STRATEGY__\", \"__CHANNEL__\", COL_CTYPE, \"__MONTH_YEAR__\", brand_file_col]\n",
    "        for keys, g in df_u.groupby(group_cols_unbranded, dropna=False):\n",
    "            ta, strat, channel, ctype_val, month_year, brand_file = keys\n",
    "            ctype_tc = title_case_str(ctype_val)\n",
    "            parts = [\n",
    "                safe_token(ta),\n",
    "                safe_token(strat),\n",
    "                safe_token(channel),\n",
    "                ctype_tc or \"NA\",\n",
    "                safe_token(brand_file),\n",
    "                safe_token(month_year),\n",
    "            ]\n",
    "            filename = \"_\".join([p for p in parts if p]) + \".csv\"\n",
    "            out_path = os.path.join(out_base, filename)\n",
    "\n",
    "            n = write_group_csv(out_path, g)\n",
    "            files_written += 1\n",
    "            rows_written += n\n",
    "            print(f\"Wrote (UNBRANDED): {out_path}  (rows: {n})\")\n",
    "\n",
    "            add_to_summary(filename, out_path, g)\n",
    "\n",
    "    # -------- Pass 3: OTHER ctypes (group WITHOUT brand_file) --------\n",
    "    others_mask = ~(branded_mask | unbranded_mask)\n",
    "    df_o = df[others_mask].copy()\n",
    "    if not df_o.empty:\n",
    "        group_cols_other = [\"TA\", \"__STRATEGY__\", \"__CHANNEL__\", COL_CTYPE, \"__MONTH_YEAR__\"]\n",
    "        for keys, g in df_o.groupby(group_cols_other, dropna=False):\n",
    "            ta, strat, channel, ctype_val, month_year = keys\n",
    "            ctype_tc = title_case_str(ctype_val)\n",
    "            parts = [safe_token(ta), safe_token(strat), safe_token(channel), ctype_tc or \"NA\", safe_token(month_year)]\n",
    "            filename = \"_\".join([p for p in parts if p]) + \".csv\"\n",
    "            out_path = os.path.join(out_base, filename)\n",
    "\n",
    "            n = write_group_csv(out_path, g)\n",
    "            files_written += 1\n",
    "            rows_written += n\n",
    "            print(f\"Wrote (OTHER): {out_path}  (rows: {n})\")\n",
    "\n",
    "            add_to_summary(filename, out_path, g)\n",
    "\n",
    "    print(f\"\\nDone. Files: {files_written}  Total rows written: {rows_written}\")\n",
    "\n",
    "    # -------- Write aggregated summary (one row per file) --------\n",
    "    if summary:\n",
    "        rows = []\n",
    "        for fname, d in summary.items():\n",
    "            rows.append({\n",
    "                \"File Name\": fname,\n",
    "                \"Full Path\": d[\"Full Path\"],\n",
    "                \"Rows\": d[\"Rows\"],\n",
    "                \"Brand (unique)\": uniq_join(d[\"Brand\"]),\n",
    "                \"Brand Indication/Campaign (unique)\": uniq_join(d[\"BIC\"]),\n",
    "                \"Campaign Type (unique)\": uniq_join(d[\"CType\"]),\n",
    "                \"Account (unique)\": uniq_join(d[\"Account\"]),\n",
    "                \"TA\": uniq_join(d[\"TA\"]),\n",
    "                \"Strategy\": uniq_join(d[\"Strategy\"]),\n",
    "                \"Channel\": uniq_join(d[\"Channel\"]),\n",
    "                \"Month_Year\": uniq_join(d[\"Month_Year\"]),\n",
    "                \"Lookup Brand File Name\": uniq_join(d[\"LookupBrandFile\"]),\n",
    "            })\n",
    "        summary_df = pd.DataFrame(rows)\n",
    "        summary_name = f\"SearchGeo_File_Summary_{df['__DATE_PARSED__'].dt.strftime('%B_%Y').iloc[0]}.csv\"\n",
    "        summary_path = os.path.join(out_base, summary_name)\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        print(f\"Summary written: {summary_path}\")\n",
    "    else:\n",
    "        print(\"No files written; skipping summary.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        conn  # noqa: F821\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"Please create a Snowflake connection named `conn` before running this script.\")\n",
    "    main(conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd363f",
   "metadata": {},
   "source": [
    "# DCM NON-DMA AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r\"C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\\Non-DMA AUD\"\n",
    "YEAR = 2025\n",
    "MONTH = 8\n",
    "TABLE_A = \"GILEAD.CUSTOM_OUTPUTS.DATAFEED_DCM_AUDIO\"\n",
    "CHANNEL_FILTER = \"Audio\"\n",
    "\n",
    "def safe_token(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    s = s.strip().replace(\" \", \"_\")\n",
    "    s = re.sub(r\"[^\\w\\-.]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"NA\"\n",
    "\n",
    "def title_case_str(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    return s.strip().lower().title()\n",
    "\n",
    "def resolve_col(df, candidates, allow_fuzzy=True):\n",
    "    \"\"\"\n",
    "    Return the actual column name from df that matches any candidate (case/space-insensitive).\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    norm = {c: re.sub(r'\\s+', '', c).lower() for c in cols}\n",
    "\n",
    "    # exact (case-insensitive)\n",
    "    for cand in candidates:\n",
    "        for c in cols:\n",
    "            if c.lower() == cand.lower():\n",
    "                return c\n",
    "\n",
    "    if allow_fuzzy:\n",
    "        cand_norms = [re.sub(r'\\s+', '', c).lower() for c in candidates]\n",
    "        for c, n in norm.items():\n",
    "            if n in cand_norms:\n",
    "                return c\n",
    "        # partial match (e.g., 'Franchise ' with trailing space)\n",
    "        for cand in cand_norms:\n",
    "            for c, n in norm.items():\n",
    "                if n == cand or n.startswith(cand) or cand.startswith(n):\n",
    "                    return c\n",
    "    return None\n",
    "\n",
    "# Define desired output schema (exact headers) with candidate source names and type\n",
    "OUTPUT_SCHEMA = [\n",
    "    # (Exact Output Header, [\"candidate names ...\"], type: \"text\"|\"num\"|\"date\")\n",
    "    (\"Franchise\", [\"FRANCHISE\",\"Franchise\",\"FRANCHISE_NAME\",\"Brand_Franchise\",\"BRAND_FRANCHISE\",\"BRAND_FRANCAHISE\",\"BRAND\"], \"text\"),\n",
    "    (\"Subsidiary\", [\"SUBSIDIARY\",\"Subsidiary\",\"SUBSIDIARY_NAME\"], \"text\"),\n",
    "    (\"Brand\", [\"BRAND\",\"Brand\"], \"text\"),\n",
    "    (\"Brand Indication/Campaign\", [\"Brand Indication/Campaign\",\"Brand Indication\",\"BRAND_INDICATION\",\"BRAND_INDICATION_CAMPAIGN\",\"CAMPAIGN_NAME\",\"Campaign Name\"], \"text\"),\n",
    "    (\"Date\", [\"DATE\",\"Date\",\"DAY\",\"Day\"], \"date\"),\n",
    "    (\"Campaign Type\", [\"Campaign Type \",\"Campaign Type\",\"CAMPAIGN_TYPE\",\"BRAND_TYPE\",\"Brand Type\"], \"text\"),\n",
    "    (\"Campaign\", [\"CAMPAIGN\",\"Campaign\",\"Campaign Name\",\"CAMPAIGN_NAME\"], \"text\"),\n",
    "    (\"Campaign ID\", [\"CAMPAIGN_ID\",\"Campaign ID\",\"CampaignId\"], \"text\"),\n",
    "    (\"Site / Publisher ID\", [\"Site/Publisher ID\",\"PUBLISHER_ID\",\"Publisher Id\",\"Site Id\"], \"text\"),\n",
    "    (\"Site / Publisher\", [\"Site/Publisher\",\"SITE/PUBLISHER\",\"PUBLISHER\",\"Publisher\",\"Site\"], \"text\"),\n",
    "    (\"DMA\", [\"DMA\",\"Dma\",\"DMA_REGION\",\"DMA Name\"], \"text\"),\n",
    "    (\"DMA Code\", [\"DMA Code\",\"DMA_CODE\",\"DMACode\",\"DMA_ID\",\"DMA Id\"], \"text\"),\n",
    "    (\"Package/Roadblock\", [\"Package/Roadblock\",\"PACKAGE/ROADBLOCK\",\"PACKAGE_ROADBLOCK\",\"PACKAGE_GROUP_NAME\",\"PACKAGE_NAME\"], \"text\"),\n",
    "    (\"Package/Roadblock ID\", [\"Package/Roadblock ID\",\"PACKAGE_ROADBLOCK_ID\",\"PACKAGE_ID\",\"Package Id\"], \"text\"),\n",
    "    (\"Placement\", [\"PLACEMENT\",\"Placement\",\"PLACEMENT_NAME\"], \"text\"),\n",
    "    (\"Placement ID\", [\"PLACEMENT_ID\",\"Placement ID\",\"PlacementId\"], \"text\"),\n",
    "    (\"Target Tactic\", [\"Target Tactic\",\"TARGET_TACTIC\",\"Tactic\",\"TACTIC\"], \"text\"),\n",
    "    (\"Creative\", [\"CREATIVE\",\"Creative\",\"CREATIVE_NAME\"], \"text\"),\n",
    "    (\"Creative ID\", [\"CREATIVE_ID\",\"Creative ID\",\"CreativeId\"], \"text\"),\n",
    "    (\"Ad\", [\"Ad\",\"AD\",\"AD_NAME\",\"Ad Name\"], \"text\"),\n",
    "    (\"Media Impressions\", [\"Media Impressions\",\"MEDIA_IMPRESSIONS\",\"IMPRESSIONS\",\"Impressions\"], \"num\"),\n",
    "    (\"Media Clicks\", [\"Media Clicks\",\"MEDIA_CLICKS\",\"CLICKS\",\"Clicks\"], \"num\"),\n",
    "    (\"Audio Plays\", [\"Audio Plays\",\"AUDIO_PLAYS\",\"PLAYS\",\"AudioPlays\"], \"num\"),\n",
    "    (\"Audio Completes\", [\"Audio Completes\",\"AUDIO_COMPLETES\",\"COMPLETES\",\"AudioCompletes\"], \"num\"),\n",
    "    (\"GMID\", [\"GMID\",\"Gmid\"], \"text\"),\n",
    "]\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {TABLE_A} a\n",
    "WHERE EXTRACT(YEAR FROM a.DATE) = {YEAR}\n",
    "  AND EXTRACT(MONTH FROM a.DATE) = {MONTH}\n",
    "  AND a.\"Channel\" = '{CHANNEL_FILTER}'\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "if df.empty:\n",
    "    print(f\"No rows for {MONTH}/{YEAR} with Channel = '{CHANNEL_FILTER}'.\")\n",
    "else:\n",
    "    # Detect Franchise / Strategy / Campaign Type used for file naming\n",
    "    franchise_col = resolve_col(df, [\"FRANCHISE\",\"Franchise\",\"FRANCHISE_NAME\",\"Brand_Franchise\",\"BRAND_FRANCHISE\",\"BRAND\"])\n",
    "    if franchise_col is None:\n",
    "        df[\"__FRANCHISE__\"] = \"NA\"\n",
    "        franchise_col = \"__FRANCHISE__\"\n",
    "\n",
    "    strat_col = resolve_col(df, [\"HCP/DTP\",\"HCP_DTP\",\"Strategy\"]) or \"__STRATEGY__\"\n",
    "    if strat_col == \"__STRATEGY__\": df[\"__STRATEGY__\"] = \"NA\"\n",
    "\n",
    "    ctype_col = resolve_col(df, [\"Campaign Type \",\"Campaign Type\",\"CAMPAIGN_TYPE\",\"BRAND_TYPE\"]) or \"__CTYPE__\"\n",
    "    if ctype_col == \"__CTYPE__\": df[\"__CTYPE__\"] = \"NA\"\n",
    "\n",
    "    # Month/year helpers\n",
    "    date_src = resolve_col(df, [\"DATE\",\"Date\",\"DAY\",\"Day\"])\n",
    "    if date_src is None:\n",
    "        raise ValueError(\"DATE column not found.\")\n",
    "    dt = pd.to_datetime(df[date_src], errors=\"coerce\")\n",
    "    if dt.isna().all():\n",
    "        raise ValueError(\"DATE column could not be parsed as dates.\")\n",
    "\n",
    "    df[\"__MONTH_NAME__\"] = dt.dt.strftime(\"%B\")\n",
    "    folder_month_label = f\"{dt.dt.strftime('%B').iloc[0]} {dt.dt.year.iloc[0]}\"\n",
    "\n",
    "    # Output folder\n",
    "    out_base = os.path.join(BASE_DIR, f\"{folder_month_label} Data Feed Delivery\", \"Non-DMA AUD\")\n",
    "    os.makedirs(out_base, exist_ok=True)\n",
    "\n",
    "    # Build a mapping {output_header: source_col_or_None}\n",
    "    source_map = {}\n",
    "    for out_name, candidates, _dtype in OUTPUT_SCHEMA:\n",
    "        found = resolve_col(df, candidates)\n",
    "        source_map[out_name] = found  # can be None\n",
    "\n",
    "    # Quick sanity prints\n",
    "    print(f\"Franchise column detected as: {franchise_col}\")\n",
    "    print(f\"Non-null franchise rows: {df[franchise_col].notna().sum()} / {len(df)}\")\n",
    "    print(\"Source column mapping (None means we will fill defaults):\")\n",
    "    for k, v in source_map.items():\n",
    "        print(f\"  {k}  <-  {v}\")\n",
    "\n",
    "    # Grouping: Franchise, HCP/DTP, Campaign Type, Month\n",
    "    group_cols = [franchise_col, strat_col, ctype_col, \"__MONTH_NAME__\"]\n",
    "\n",
    "    files_written = 0\n",
    "    rows_written = 0\n",
    "\n",
    "    for keys, g in df.groupby(group_cols, dropna=False):\n",
    "        franchise, strat, ctype_val, month_name = keys\n",
    "\n",
    "        ctype_tc = title_case_str(ctype_val)\n",
    "        strat_up = (str(strat).strip().upper() or \"NA\")\n",
    "\n",
    "        # Prepare output frame with exact columns in required order\n",
    "        out_frames = {}\n",
    "        for out_name, _cands, dtype in OUTPUT_SCHEMA:\n",
    "            src = source_map[out_name]\n",
    "            if src is not None and src in g.columns:\n",
    "                col = g[src].copy()\n",
    "            else:\n",
    "                # create default column\n",
    "                if dtype == \"num\":\n",
    "                    col = pd.Series(0, index=g.index, dtype=\"Int64\")\n",
    "                elif dtype == \"date\":\n",
    "                    # coerce from current group date source if present\n",
    "                    if date_src in g.columns:\n",
    "                        col = pd.to_datetime(g[date_src], errors=\"coerce\")\n",
    "                    else:\n",
    "                        col = pd.NaT\n",
    "                else:\n",
    "                    col = pd.Series(\"NA\", index=g.index, dtype=\"object\")\n",
    "            out_frames[out_name] = col\n",
    "\n",
    "        g_out = pd.DataFrame(out_frames)\n",
    "\n",
    "        # Ensure DATE is date-like and not object\n",
    "        if not pd.api.types.is_datetime64_any_dtype(g_out[\"Date\"]):\n",
    "            g_out[\"Date\"] = pd.to_datetime(g_out[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "        # Final sort + write\n",
    "        g_out = g_out.sort_values(by=[\"Date\"], na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "        # Franchise_HCP-or-DTP_AUD_Branded-or-Unbranded_Month_2025_Non-DMA.csv\n",
    "        filename = \"_\".join([\n",
    "            safe_token(franchise),\n",
    "            safe_token(strat_up),\n",
    "            \"AUD\",\n",
    "            safe_token(ctype_tc if ctype_tc else \"NA\"),\n",
    "            safe_token(month_name),\n",
    "            str(YEAR),\n",
    "            \"Non-DMA\"\n",
    "        ]) + \".csv\"\n",
    "\n",
    "        out_path = os.path.join(out_base, filename)\n",
    "        g_out.to_csv(out_path, index=False)\n",
    "\n",
    "        files_written += 1\n",
    "        rows_written += len(g_out)\n",
    "        print(f\"Wrote: {out_path}  (rows: {len(g_out)})\")\n",
    "\n",
    "    print(f\"\\nDone. Files: {files_written}  Total rows written: {rows_written}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f478856",
   "metadata": {},
   "source": [
    "# DCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabea24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r\"C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\\DCM\"\n",
    "TABLE_A = \"DATAFEED_DCM\"\n",
    "LOOKUP_TABLE = \"GILEAD_LOOKUP_DCM_BRAND_NAME\"   \n",
    "\n",
    "YEAR = 2025\n",
    "MONTH = 8\n",
    "SELECTED_CHANNELS = [\"Audio\", \"CTV\", \"Custom\", \"Display\", \"FEP\", \"Newsletter\", \"OLV\"]\n",
    "\n",
    "CHUNKSIZE = 250_000\n",
    "EXCEL_ROW_CAP = 1_048_574\n",
    "\n",
    "# Channels where we should NOT include the \"Video Completes\" column in the exported CSV\n",
    "NONVIDEO_CHANNELS = {\"display\", \"newsletter\",\"custom\"}\n",
    "\n",
    "# Channels that should use Publisher BEFORE Publisher ID in the schema\n",
    "CHANNELS_PUBLISHER_FIRST = {\"custom\", \"display\", \"newsletter\"}\n",
    "# ----------------------------------------\n",
    "\n",
    "def safe_token(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    s = s.strip().replace(\" \", \"_\")\n",
    "    s = re.sub(r\"[^\\w\\-.]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"NA\"\n",
    "\n",
    "def title_case_str(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    return s.strip().lower().title()\n",
    "\n",
    "def resolve_col(df, candidates, allow_fuzzy=True):\n",
    "    cols = list(df.columns)\n",
    "    norm = {c: re.sub(r\"\\s+\", \"\", c).lower() for c in cols}\n",
    "    # exact (case-insensitive)\n",
    "    for cand in candidates:\n",
    "        for c in cols:\n",
    "            if c.lower() == cand.lower():\n",
    "                return c\n",
    "    if allow_fuzzy:\n",
    "        cand_norms = [re.sub(r\"\\s+\", \"\", c).lower() for c in candidates]\n",
    "        # whitespace-insensitive\n",
    "        for c, n in norm.items():\n",
    "            if n in cand_norms:\n",
    "                return c\n",
    "        # prefix-ish matches\n",
    "        for cand in cand_norms:\n",
    "            for c, n in norm.items():\n",
    "                if n == cand or n.startswith(cand) or cand.startswith(n):\n",
    "                    return c\n",
    "    return None\n",
    "\n",
    "def num_files_from_count_alteryx(count: int) -> int:\n",
    "    ratio = count / EXCEL_ROW_CAP\n",
    "    if ratio <= 1: return 1\n",
    "    elif ratio <= 2: return 3\n",
    "    else: return min(10, int(math.ceil(ratio)) + 1)\n",
    "\n",
    "def month_day_ranges(year: int, month: int, n: int):\n",
    "    last = calendar.monthrange(year, month)[1]\n",
    "    if n == 1: return [(1, last)]\n",
    "    if n == 2: return [(1, 15), (16, last)]\n",
    "    if n == 3: return [(1, 10), (11, 20), (21, last)]\n",
    "    if n == 4: return [(1, 7), (8, 14), (15, 21), (22, last)]\n",
    "    if n == 5: return [(1, 6), (7, 12), (13, 18), (19, 24), (25, last)]\n",
    "    # balanced split fallback\n",
    "    edges = [1]\n",
    "    for k in range(1, n):\n",
    "        d = math.floor(1 + (last - 1) * (k / n))\n",
    "        if d <= edges[-1]: d = edges[-1] + 1\n",
    "        edges.append(min(d, last))\n",
    "    edges.append(last + 1)\n",
    "    ranges = []\n",
    "    for i in range(n):\n",
    "        start = edges[i]\n",
    "        end = min(edges[i+1] - 1, last)\n",
    "        if end < start: end = start\n",
    "        ranges.append((start, end))\n",
    "    ranges[-1] = (ranges[-1][0], last)\n",
    "    return ranges\n",
    "\n",
    "def day_of_month(series):\n",
    "    s = pd.to_datetime(series, errors=\"coerce\")\n",
    "    return s.dt.day\n",
    "\n",
    "# --------- Output Schemas ----------\n",
    "OUTPUT_SCHEMA_STANDARD = [\n",
    "    (\"Franchise\", [\"FRANCHISE\",\"Franchise\",\"FRANCHISE_NAME\",\"Brand_Franchise\",\"BRAND_FRANCHISE\",\"BRAND_FRANCAHISE\",\"BRAND\"], \"text\"),\n",
    "    (\"Subsidiary\", [\"SUBSIDIARY\",\"Subsidiary\",\"SUBSIDIARY_NAME\"], \"text\"),\n",
    "    (\"Brand\", [\"BRAND\",\"Brand\"], \"text\"),\n",
    "    (\"Brand Indication/Campaign\", [\"Brand Indication/Campaign\",\"Brand Indication\",\"BRAND_INDICATION\",\"BRAND_INDICATION_CAMPAIGN\",\"CAMPAIGN_NAME\",\"Campaign Name\"], \"text\"),\n",
    "    (\"Date\", [\"DATE\",\"Date\",\"DAY\",\"Day\"], \"date\"),\n",
    "    (\"Campaign Type\", [\"Campaign Type \",\"Campaign Type\",\"CAMPAIGN_TYPE\",\"BRAND_TYPE\",\"Brand Type\"], \"text\"),\n",
    "    (\"Campaign\", [\"CAMPAIGN\",\"Campaign\",\"Campaign Name\",\"CAMPAIGN_NAME\"], \"text\"),\n",
    "    (\"Campaign ID\", [\"CAMPAIGN_ID\",\"Campaign ID\",\"CampaignId\"], \"text\"),\n",
    "    (\"Site / Publisher ID\", [\"Site/Publisher ID\",\"SITE/PUBLISHER ID\",\"PUBLISHER_ID\",\"Publisher Id\",\"Site Id\",\"SITE_ID\"], \"text\"),\n",
    "    (\"Site / Publisher\", [\"Site/Publisher\",\"SITE/PUBLISHER\",\"PUBLISHER\",\"Publisher\",\"Site\"], \"text\"),\n",
    "    (\"DMA\", [\"DMA\",\"Dma\",\"DMA_REGION\",\"DMA Name\"], \"text\"),\n",
    "    (\"DMA Code\", [\"DMA Code\",\"DMA_CODE\",\"DMACode\",\"DMA_ID\",\"DMA Id\"], \"text\"),\n",
    "    (\"Package/Roadblock\", [\"Package/Roadblock\",\"PACKAGE/ROADBLOCK\",\"PACKAGE_ROADBLOCK\",\"PACKAGE_GROUP_NAME\",\"PACKAGE_NAME\"], \"text\"),\n",
    "    (\"Package/Roadblock ID\", [\"Package/Roadblock ID\",\"PACKAGE_ROADBLOCK_ID\",\"PACKAGE_ID\",\"Package Id\"], \"text\"),\n",
    "    (\"Placement\", [\"PLACEMENT\",\"Placement\",\"PLACEMENT_NAME\"], \"text\"),\n",
    "    (\"Placement ID\", [\"PLACEMENT_ID\",\"Placement ID\",\"PlacementId\"], \"text\"),\n",
    "    (\"Target Tactic\", [\"Target Tactic\",\"TARGET_TACTIC\",\"Tactic\",\"TACTIC\"], \"text\"),\n",
    "    (\"Creative\", [\"CREATIVE\",\"Creative\",\"CREATIVE_NAME\"], \"text\"),\n",
    "    (\"Creative ID\", [\"CREATIVE_ID\",\"Creative ID\",\"CreativeId\"], \"text\"),\n",
    "    (\"Ad\", [\"Ad\",\"AD\",\"AD_NAME\",\"Ad Name\"], \"text\"),\n",
    "    (\"Media Impressions\", [\"Media Impressions\",\"MEDIA_IMPRESSIONS\",\"IMPRESSIONS\",\"Impressions\"], \"num\"),\n",
    "    (\"Media Clicks\", [\"Media Clicks\",\"MEDIA_CLICKS\",\"CLICKS\",\"Clicks\"], \"num\"),\n",
    "    (\"Video Views\", [\"Video Views\",\"VIDEO_VIEWS\",\"VIDEO VIEWS\",\"Views\",\"VIEWS\",\"VIDEO_STARTS\"], \"num\"),\n",
    "    (\"Video Completes\", [\"Video Completes\",\"VIDEO_COMPLETES\",\"VIDEO COMPLETES\",\"Completions\",\"COMPLETIONS\",\"VIDEO_COMPLETIONS\"], \"num\"),\n",
    "    (\"GMID\", [\"GMID\",\"Gmid\"], \"text\"),\n",
    "]\n",
    "\n",
    "# Publisher-first schema (Name before ID) for Custom/Display/Newsletter\n",
    "OUTPUT_SCHEMA_PUBLISHER_FIRST = [\n",
    "    (\"Franchise\", [\"FRANCHISE\",\"Franchise\",\"FRANCHISE_NAME\",\"Brand_Franchise\",\"BRAND_FRANCHISE\",\"BRAND_FRANCAHISE\",\"BRAND\"], \"text\"),\n",
    "    (\"Subsidiary\", [\"SUBSIDIARY\",\"Subsidiary\",\"SUBSIDIARY_NAME\"], \"text\"),\n",
    "    (\"Brand\", [\"BRAND\",\"Brand\"], \"text\"),\n",
    "    (\"Brand Indication/Campaign\", [\"Brand Indication/Campaign\",\"Brand Indication\",\"BRAND_INDICATION\",\"BRAND_INDICATION_CAMPAIGN\",\"CAMPAIGN_NAME\",\"Campaign Name\"], \"text\"),\n",
    "    (\"Date\", [\"DATE\",\"Date\",\"DAY\",\"Day\"], \"date\"),\n",
    "    (\"Campaign Type\", [\"Campaign Type \",\"Campaign Type\",\"CAMPAIGN_TYPE\",\"BRAND_TYPE\",\"Brand Type\"], \"text\"),\n",
    "    (\"Campaign\", [\"CAMPAIGN\",\"Campaign\",\"Campaign Name\",\"CAMPAIGN_NAME\"], \"text\"),\n",
    "    (\"Campaign ID\", [\"CAMPAIGN_ID\",\"Campaign ID\",\"CampaignId\"], \"text\"),\n",
    "    (\"Site / Publisher\", [\"Site/Publisher\",\"SITE/PUBLISHER\",\"PUBLISHER\",\"Publisher\",\"Site\"], \"text\"),\n",
    "    (\"Site / Publisher ID\", [\"Site/Publisher ID\",\"SITE/PUBLISHER ID\",\"PUBLISHER_ID\",\"Publisher Id\",\"Site Id\",\"SITE_ID\"], \"text\"),\n",
    "    (\"DMA\", [\"DMA\",\"Dma\",\"DMA_REGION\",\"DMA Name\"], \"text\"),\n",
    "    (\"DMA Code\", [\"DMA Code\",\"DMA_CODE\",\"DMACode\",\"DMA_ID\",\"DMA Id\"], \"text\"),\n",
    "    (\"Package/Roadblock\", [\"Package/Roadblock\",\"PACKAGE/ROADBLOCK\",\"PACKAGE_ROADBLOCK\",\"PACKAGE_GROUP_NAME\",\"PACKAGE_NAME\"], \"text\"),\n",
    "    (\"Package/Roadblock ID\", [\"Package/Roadblock ID\",\"PACKAGE_ROADBLOCK_ID\",\"PACKAGE_ID\",\"Package Id\"], \"text\"),\n",
    "    (\"Placement\", [\"PLACEMENT\",\"Placement\",\"PLACEMENT_NAME\"], \"text\"),\n",
    "    (\"Placement ID\", [\"PLACEMENT_ID\",\"Placement ID\",\"PlacementId\"], \"text\"),\n",
    "    (\"Target Tactic\", [\"Target Tactic\",\"TARGET_TACTIC\",\"Tactic\",\"TACTIC\"], \"text\"),\n",
    "    (\"Creative\", [\"CREATIVE\",\"Creative\",\"CREATIVE_NAME\"], \"text\"),\n",
    "    (\"Creative ID\", [\"CREATIVE_ID\",\"Creative ID\",\"CreativeId\"], \"text\"),\n",
    "    (\"Ad\", [\"Ad\",\"AD\",\"AD_NAME\",\"Ad Name\"], \"text\"),\n",
    "    (\"Media Impressions\", [\"Media Impressions\",\"MEDIA_IMPRESSIONS\",\"IMPRESSIONS\",\"Impressions\"], \"num\"),\n",
    "    (\"Media Clicks\", [\"Media Clicks\",\"MEDIA_CLICKS\",\"CLICKS\",\"Clicks\"], \"num\"),\n",
    "    (\"Video Views\", [\"Video Views\",\"VIDEO_VIEWS\",\"VIDEO VIEWS\",\"Views\",\"VIEWS\",\"VIDEO_STARTS\"], \"num\"),\n",
    "    (\"Video Completes\", [\"Video Completes\",\"VIDEO_COMPLETES\",\"VIDEO COMPLETES\",\"Completions\",\"COMPLETIONS\",\"VIDEO_COMPLETIONS\"], \"num\"),\n",
    "    (\"GMID\", [\"GMID\",\"Gmid\"], \"text\"),\n",
    "]\n",
    "\n",
    "month_start = dt.date(YEAR, MONTH, 1)\n",
    "next_month_start = dt.date(YEAR + (MONTH == 12), (MONTH % 12) + 1, 1)\n",
    "\n",
    "chan_list = [c.replace(\"'\", \"''\") for c in SELECTED_CHANNELS]\n",
    "chan_in = \", \".join(f\"'{c}'\" for c in chan_list)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {TABLE_A} a\n",
    "WHERE a.DATE >= TO_DATE('{month_start:%Y-%m-%d}')\n",
    "  AND a.DATE <  TO_DATE('{next_month_start:%Y-%m-%d}')\n",
    "  AND a.\"Channel\" IN ({chan_in})\n",
    "\"\"\"\n",
    "\n",
    "# ----------------- Load lookup once (Campaign  Brand File Name, TA) -----------------\n",
    "lookup_df = pd.read_sql(f\"SELECT * FROM {LOOKUP_TABLE}\", conn)\n",
    "\n",
    "lkp_campaign_col = resolve_col(lookup_df, [\"CAMPAIGN_NAME\",\"Campaign Name\",\"Campaign\",\"BRAND_INDICATION_CAMPAIGN\",\"Brand Indication/Campaign\"])\n",
    "if lkp_campaign_col is None:\n",
    "    raise ValueError(f\"Could not find campaign column in {LOOKUP_TABLE}\")\n",
    "\n",
    "lkp_brandfile_col = resolve_col(lookup_df, [\"Brand File Name\",\"BRAND_FILE_NAME\",\"Brand_File_Name\",\"BrandFileName\"])\n",
    "if lkp_brandfile_col is None:\n",
    "    raise ValueError(f\"Could not find 'Brand File Name' column in {LOOKUP_TABLE}\")\n",
    "\n",
    "lkp_ta_col = resolve_col(lookup_df, [\"TA\",\"Therapeutic Area\",\"TA_NAME\"])\n",
    "if lkp_ta_col is None:\n",
    "    raise ValueError(f\"Could not find TA column in {LOOKUP_TABLE}\")\n",
    "\n",
    "# Deduplicate so mapping is deterministic (keep first non-null)\n",
    "lookup_df = lookup_df[[lkp_campaign_col, lkp_brandfile_col, lkp_ta_col]].copy()\n",
    "lookup_df = lookup_df.dropna(subset=[lkp_campaign_col]).drop_duplicates(subset=[lkp_campaign_col], keep=\"first\")\n",
    "\n",
    "BRAND_NAME_MAP = dict(zip(lookup_df[lkp_campaign_col].astype(str), lookup_df[lkp_brandfile_col].astype(str)))\n",
    "TA_MAP = dict(zip(lookup_df[lkp_campaign_col].astype(str), lookup_df[lkp_ta_col].astype(str)))\n",
    "\n",
    "folder_month_label = f\"{calendar.month_name[MONTH]} {YEAR}\"\n",
    "\n",
    "def ensure_out_path(franchise, strat, ctype_val, month_name, channel_val,\n",
    "                    ta_val=None, brand_file_name=None, start_day=None, end_day=None):\n",
    "    \"\"\"\n",
    "    Build the output path. TA (from lookup) is included in filename.\n",
    "    FranchiseTA de-duplication:\n",
    "      - If identical (case-insensitive), keep one.\n",
    "      - If they share the same 'family head' (before '_' or '-'), or one is a prefix of the other,\n",
    "        prefer TA when it is the 'family' form (e.g., ONCOLOGY + ONC_Trodelvy  keep ONC_Trodelvy).\n",
    "        Otherwise keep the shorter token.\n",
    "      - Else keep both [Franchise, TA].\n",
    "    If Campaign Type == UNBRANDED, include brand_file_name in filename.\n",
    "    \"\"\"\n",
    "    channel_folder = (str(channel_val).strip() or \"Unknown\")\n",
    "    safe_channel_folder = safe_token(channel_folder) or \"Unknown\"\n",
    "    out_base = os.path.join(\n",
    "        BASE_DIR,\n",
    "        channel_folder if safe_channel_folder == channel_folder else safe_channel_folder,\n",
    "        f\"{folder_month_label} Data Feed Delivery\"\n",
    "    )\n",
    "    os.makedirs(out_base, exist_ok=True)\n",
    "\n",
    "    ctype_tc = title_case_str(ctype_val)  # \"Branded\" / \"Unbranded\" / etc.\n",
    "    strat_up = (str(strat).strip().upper() or \"NA\")\n",
    "    channel_token = safe_token(channel_val).upper() or \"CHANNEL\"\n",
    "    month_token = safe_token(month_name)\n",
    "\n",
    "    # Clean tokens\n",
    "    franchise_token = safe_token(franchise)\n",
    "    ta_token = safe_token(ta_val)\n",
    "\n",
    "    def head(s: str) -> str:\n",
    "        return re.split(r\"[_\\-]\", s.upper())[0] if s else \"\"\n",
    "\n",
    "    # Smart FranchiseTA merge\n",
    "    if franchise_token and ta_token:\n",
    "        fU, tU = franchise_token.upper(), ta_token.upper()\n",
    "        fH, tH = head(franchise_token), head(ta_token)\n",
    "        if fU == tU:\n",
    "            lead_parts = [franchise_token]\n",
    "        elif fH == tH or fH.startswith(tH) or tH.startswith(fH):\n",
    "            if \"_\" in ta_token or \"-\" in ta_token:\n",
    "                lead_parts = [ta_token]\n",
    "            else:\n",
    "                lead_parts = [franchise_token] if len(franchise_token) < len(ta_token) else [ta_token]\n",
    "        else:\n",
    "            lead_parts = [franchise_token, ta_token]\n",
    "    elif franchise_token:\n",
    "        lead_parts = [franchise_token]\n",
    "    elif ta_token:\n",
    "        lead_parts = [ta_token]\n",
    "    else:\n",
    "        lead_parts = [\"NA\"]\n",
    "\n",
    "    parts = lead_parts + [\n",
    "        safe_token(strat_up),\n",
    "        channel_token,\n",
    "        safe_token(ctype_tc if ctype_tc else \"NA\"),\n",
    "    ]\n",
    "\n",
    "    # Only for UNBRANDED include Brand File Name in filename.\n",
    "    if ctype_tc == \"Unbranded\" and brand_file_name:\n",
    "        parts.append(safe_token(brand_file_name))\n",
    "\n",
    "    if start_day is None:\n",
    "        parts.extend([month_token, str(YEAR)])\n",
    "    else:\n",
    "        parts.append(f\"{month_token}_{start_day}-{end_day}_{YEAR}\")\n",
    "\n",
    "    filename = \"_\".join(parts) + \".csv\"\n",
    "    return os.path.join(out_base, filename)\n",
    "\n",
    "# ----------------- PASS 1: counts per group (Brand File Name + TA from lookup) -----------------\n",
    "group_counts = {}\n",
    "\n",
    "for chunk in pd.read_sql(query, conn, chunksize=CHUNKSIZE):\n",
    "    # Detect key columns\n",
    "    channel_col  = resolve_col(chunk, [\"Channel\",\"CHANNEL\",\"channel\"])  or \"__CHANNEL__\"\n",
    "    if channel_col == \"__CHANNEL__\": chunk[\"__CHANNEL__\"] = \"Unknown\"\n",
    "\n",
    "    franchise_col = resolve_col(chunk, [\"FRANCHISE\",\"Franchise\",\"FRANCHISE_NAME\",\"Brand_Franchise\",\"BRAND_FRANCHISE\",\"BRAND\"]) or \"__FRANCHISE__\"\n",
    "    if franchise_col == \"__FRANCHISE__\": chunk[\"__FRANCHISE__\"] = \"NA\"\n",
    "\n",
    "    strat_col = resolve_col(chunk, [\"HCP/DTP\",\"HCP_DTP\",\"Strategy\"]) or \"__STRATEGY__\"\n",
    "    if strat_col == \"__STRATEGY__\": chunk[\"__STRATEGY__\"] = \"NA\"\n",
    "\n",
    "    ctype_col = resolve_col(chunk, [\"Campaign Type \",\"Campaign Type\",\"CAMPAIGN_TYPE\",\"BRAND_TYPE\",\"Brand Type\"]) or \"__CTYPE__\"\n",
    "    if ctype_col == \"__CTYPE__\": chunk[\"__CTYPE__\"] = \"NA\"\n",
    "\n",
    "    date_src = resolve_col(chunk, [\"DATE\",\"Date\",\"DAY\",\"Day\"])\n",
    "    if date_src is None:\n",
    "        raise ValueError(\"DATE column not found in chunk.\")\n",
    "    dts = pd.to_datetime(chunk[date_src], errors=\"coerce\")\n",
    "    if dts.isna().all():\n",
    "        raise ValueError(\"DATE could not be parsed in chunk.\")\n",
    "\n",
    "    # Map via campaign to Brand File Name and TA (from lookup)\n",
    "    campaign_col = resolve_col(chunk, [\"CAMPAIGN_NAME\",\"Campaign Name\",\"Campaign\",\"BRAND_INDICATION_CAMPAIGN\",\"Brand Indication/Campaign\"]) or \"__CAMPAIGN__\"\n",
    "    if campaign_col == \"__CAMPAIGN__\":\n",
    "        chunk[\"__CAMPAIGN__\"] = \"NA\"\n",
    "        campaign_col = \"__CAMPAIGN__\"\n",
    "\n",
    "    chunk[\"__BRAND_FILE_NAME__\"] = chunk[campaign_col].astype(str).map(BRAND_NAME_MAP).fillna(\"NA\")\n",
    "    chunk[\"__TA__\"] = chunk[campaign_col].astype(str).map(TA_MAP).fillna(\"NA\")\n",
    "    chunk[\"__MONTH_NAME__\"] = calendar.month_name[MONTH]\n",
    "\n",
    "    # Group by TA (from lookup) and brand file name\n",
    "    grp = chunk.groupby([franchise_col, strat_col, ctype_col, \"__MONTH_NAME__\", channel_col, \"__BRAND_FILE_NAME__\", \"__TA__\"], dropna=False).size()\n",
    "    for key, cnt in grp.items():\n",
    "        group_counts[key] = group_counts.get(key, 0) + int(cnt)\n",
    "\n",
    "if not group_counts:\n",
    "    print(\"No rows found for the chosen month + channels.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "group_files_needed = {k: num_files_from_count_alteryx(v) for k, v in group_counts.items()}\n",
    "group_ranges = {k: month_day_ranges(YEAR, MONTH, group_files_needed[k]) for k in group_counts}\n",
    "\n",
    "# ----------------- PASS 2: write incrementally -----------------\n",
    "files_written = set()\n",
    "rows_written_total = 0\n",
    "\n",
    "for chunk in pd.read_sql(query, conn, chunksize=CHUNKSIZE):\n",
    "    channel_col  = resolve_col(chunk, [\"Channel\",\"CHANNEL\",\"channel\"])  or \"__CHANNEL__\"\n",
    "    if channel_col == \"__CHANNEL__\": chunk[\"__CHANNEL__\"] = \"Unknown\"\n",
    "\n",
    "    franchise_col = resolve_col(chunk, [\"FRANCHISE\",\"Franchise\",\"FRANCHISE_NAME\",\"Brand_Franchise\",\"BRAND_FRANCHISE\",\"BRAND\"]) or \"__FRANCHISE__\"\n",
    "    if franchise_col == \"__FRANCHISE__\": chunk[\"__FRANCHISE__\"] = \"NA\"\n",
    "\n",
    "    strat_col = resolve_col(chunk, [\"HCP/DTP\",\"HCP_DTP\",\"Strategy\"]) or \"__STRATEGY__\"\n",
    "    if strat_col == \"__STRATEGY__\": chunk[\"__STRATEGY__\"] = \"NA\"\n",
    "\n",
    "    ctype_col = resolve_col(chunk, [\"Campaign Type \",\"Campaign Type\",\"CAMPAIGN_TYPE\",\"BRAND_TYPE\",\"Brand Type\"]) or \"__CTYPE__\"\n",
    "    if ctype_col == \"__CTYPE__\": chunk[\"__CTYPE__\"] = \"NA\"\n",
    "\n",
    "    date_src = resolve_col(chunk, [\"DATE\",\"Date\",\"DAY\",\"Day\"])\n",
    "    dts = pd.to_datetime(chunk[date_src], errors=\"coerce\")\n",
    "    chunk[\"__MONTH_NAME__\"] = calendar.month_name[MONTH]\n",
    "    chunk[\"__DAY__\"] = dts.dt.day\n",
    "\n",
    "    # Map via campaign to Brand File Name and TA (from lookup)\n",
    "    campaign_col = resolve_col(chunk, [\"CAMPAIGN_NAME\",\"Campaign Name\",\"Campaign\",\"BRAND_INDICATION_CAMPAIGN\",\"Brand Indication/Campaign\"]) or \"__CAMPAIGN__\"\n",
    "    if campaign_col == \"__CAMPAIGN__\":\n",
    "        chunk[\"__CAMPAIGN__\"] = \"NA\"\n",
    "        campaign_col = \"__CAMPAIGN__\"\n",
    "\n",
    "    chunk[\"__BRAND_FILE_NAME__\"] = chunk[campaign_col].astype(str).map(BRAND_NAME_MAP).fillna(\"NA\")\n",
    "    chunk[\"__TA__\"] = chunk[campaign_col].astype(str).map(TA_MAP).fillna(\"NA\")\n",
    "\n",
    "    # Determine which schema to use for this channel\n",
    "    # NOTE: keys are per-group, so we pick schema inside the group loop below.\n",
    "    source_map_cache = {}\n",
    "\n",
    "    # Group (includes TA from lookup)\n",
    "    for keys, g in chunk.groupby([franchise_col, strat_col, ctype_col, \"__MONTH_NAME__\", channel_col, \"__BRAND_FILE_NAME__\", \"__TA__\"], dropna=False):\n",
    "        franchise, strat, ctype_val, month_name, channel_val, brand_file_name, ta_val = keys\n",
    "\n",
    "        # Choose schema based on channel\n",
    "        ch_low = str(channel_val).strip().lower()\n",
    "        schema = OUTPUT_SCHEMA_PUBLISHER_FIRST if ch_low in CHANNELS_PUBLISHER_FIRST else OUTPUT_SCHEMA_STANDARD\n",
    "\n",
    "        # Build/Reuse source_map for this schema key (to avoid re-resolving every group)\n",
    "        schema_key = tuple([c[0] for c in schema])\n",
    "        if (ch_low, schema_key) not in source_map_cache:\n",
    "            smap = {}\n",
    "            for out_name, candidates, _dtype in schema:\n",
    "                smap[out_name] = resolve_col(chunk, candidates)\n",
    "            source_map_cache[(ch_low, schema_key)] = smap\n",
    "        source_map = source_map_cache[(ch_low, schema_key)]\n",
    "\n",
    "        n_files = group_files_needed.get(keys, 1)\n",
    "        ranges = group_ranges.get(keys, [(1, calendar.monthrange(YEAR, MONTH)[1])])\n",
    "\n",
    "        # Build standardized dataframe following the chosen schema\n",
    "        out_frames = {}\n",
    "        for out_name, _cands, dtype in schema:\n",
    "            src = source_map[out_name]\n",
    "            if src is not None and src in g.columns:\n",
    "                col = g[src].copy()\n",
    "            else:\n",
    "                if dtype == \"num\":\n",
    "                    col = pd.Series(0, index=g.index, dtype=\"Int64\")\n",
    "                elif dtype == \"date\":\n",
    "                    col = pd.to_datetime(g[date_src], errors=\"coerce\")\n",
    "                else:\n",
    "                    col = pd.Series(\"NA\", index=g.index, dtype=\"object\")\n",
    "            out_frames[out_name] = col\n",
    "        g_base = pd.DataFrame(out_frames)\n",
    "\n",
    "        # Remove \"Video Completes\" for Display & Newsletter exports\n",
    "        nonvideo = ch_low in NONVIDEO_CHANNELS\n",
    "        if nonvideo:\n",
    "            g_base = g_base.drop(columns=[\"Video Completes\"], errors=\"ignore\")\n",
    "\n",
    "        # Sort by standardized \"Date\" column (not \"DATE\")\n",
    "        if \"Date\" in g_base.columns:\n",
    "            g_base.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "        # Write file(s)\n",
    "        if n_files == 1:\n",
    "            out_path = ensure_out_path(\n",
    "                franchise, strat, ctype_val, month_name, channel_val,\n",
    "                ta_val=ta_val,\n",
    "                brand_file_name=brand_file_name if title_case_str(ctype_val) == \"Unbranded\" else None\n",
    "            )\n",
    "            header = not os.path.exists(out_path)\n",
    "            g_base.to_csv(out_path, index=False, mode=\"a\", header=header)\n",
    "            files_written.add(out_path)\n",
    "            rows_written_total += len(g_base)\n",
    "        else:\n",
    "            dom = g[\"__DAY__\"]\n",
    "            for (start_day, end_day) in ranges:\n",
    "                part = g_base.loc[(dom >= start_day) & (dom <= end_day)]\n",
    "                if part.empty:\n",
    "                    continue\n",
    "                out_path = ensure_out_path(\n",
    "                    franchise, strat, ctype_val, month_name, channel_val,\n",
    "                    ta_val=ta_val,\n",
    "                    brand_file_name=brand_file_name if title_case_str(ctype_val) == \"Unbranded\" else None,\n",
    "                    start_day=start_day, end_day=end_day\n",
    "                )\n",
    "                header = not os.path.exists(out_path)\n",
    "                part.to_csv(out_path, index=False, mode=\"a\", header=header)\n",
    "                files_written.add(out_path)\n",
    "                rows_written_total += len(part)\n",
    "\n",
    "print(f\"Done. Files: {len(files_written)}  Total rows written: {rows_written_total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb6864",
   "metadata": {},
   "source": [
    "# SOCIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r\"C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\\Social\"\n",
    "YEAR = 2025\n",
    "MONTH = 8\n",
    "\n",
    "TABLE_A = \"GILEAD.CUSTOM_OUTPUTS.DATAFEED_SOCIAL\"\n",
    "TABLE_B = \"GILEAD.CUSTOM_OUTPUTS.GILEAD_LOOKUP_SOCIAL_BRAND_NAME\"\n",
    "\n",
    "\n",
    "# The exact headers (and order) you want in CSVs\n",
    "DESIRED_HEADERS = [\n",
    "    \"Franchise\",\n",
    "    \"Subsidiary\",\n",
    "    \"Brand\",\n",
    "    \"Brand Indication/Campaign\",\n",
    "    \"Date\",\n",
    "    \"Week Start\",\n",
    "    \"Week Sort\",\n",
    "    \"Campaign Type\",\n",
    "    \"Campaign\",\n",
    "    \"Campaign ID\",\n",
    "    \"Site / Publisher\",\n",
    "    \"Site / Publisher ID\",\n",
    "    \"DMA\",\n",
    "    \"DMA Code\",\n",
    "    \"Ad Group\",\n",
    "    \"Ad Group ID\",\n",
    "    \"Target Audience\",\n",
    "    \"Ad\",\n",
    "    \"Ad ID\",\n",
    "    \"Media Impressions\",\n",
    "    \"Media Clicks\",\n",
    "    \"Click through Rate\",\n",
    "    \"Engagement\",\n",
    "    \"Media Cost(USD)\",\n",
    "    \"Video Views\",\n",
    "    \"Video Completes\",\n",
    "    \"GMID\",\n",
    "]\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def safe_token(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    s = s.strip().replace(\" \", \"_\")\n",
    "    s = re.sub(r\"[^\\w\\-.]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"NA\"\n",
    "\n",
    "def title_case_str(s) -> str:\n",
    "    s = \"\" if s is None or (isinstance(s, float) and pd.isna(s)) else str(s)\n",
    "    return s.strip().lower().title()\n",
    "\n",
    "def derive_strategy_from_fields(site_pub_val, campaign_val) -> str:\n",
    "    s = \"\" if site_pub_val is None or (isinstance(site_pub_val, float) and pd.isna(site_pub_val)) else str(site_pub_val).lower()\n",
    "    c = \"\" if campaign_val  is None or (isinstance(campaign_val,  float) and pd.isna(campaign_val))  else str(campaign_val).lower()\n",
    "    if \"dtc\" in s or \"_dtc_\" in c: return \"DTC\"\n",
    "    if \"dtp\" in s or \"_dtp_\" in c: return \"DTP\"\n",
    "    if \"hcp\" in s or \"_hcp_\" in c: return \"HCP\"\n",
    "    return \"NA\"\n",
    "\n",
    "# ========= QUERY =========\n",
    "# Harden JOIN (TRIM + UPPER), and coalesce TA to Brand when lookup missing\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  a.\"Franchise\",\n",
    "  a.\"Subsidiary\",\n",
    "  a.\"Brand\",\n",
    "  a.\"Brand Indication/Campaign\",\n",
    "  a.\"Date\",\n",
    "  a.\"Week Start\",\n",
    "  a.\"Week Sort\",\n",
    "  a.\"Campaign Type\",\n",
    "  a.\"Campaign\",\n",
    "  a.\"Campaign ID\",\n",
    "  a.\"Site/Publisher\",\n",
    "  a.\"Site/Publisher ID\",\n",
    "  a.\"DMA\",\n",
    "  a.\"DMA Code\",\n",
    "  a.\"Ad Group\",\n",
    "  a.\"Ad Group ID\",\n",
    "  a.\"Target Audience\",\n",
    "  a.\"Ad\",\n",
    "  a.\"Ad ID\",\n",
    "  a.\"Media Impressions\",\n",
    "  a.\"Media Clicks\",\n",
    "  a.\"Click Through Rate\",\n",
    "  a.\"Engagements\",\n",
    "  a.\"Media Cost(USD)\",\n",
    "  a.\"Video Views\",\n",
    "  a.\"Video Completes\",\n",
    "  COALESCE(NULLIF(b.TA, ''), a.\"Brand\") AS TA,\n",
    "  b.BRAND_FILE_NAME,\n",
    "  b.STRATEGY AS LOOKUP_STRATEGY\n",
    "FROM {TABLE_A} a\n",
    "LEFT JOIN {TABLE_B} b\n",
    "  ON UPPER(TRIM(a.\"Campaign\")) = UPPER(TRIM(b.CAMPAIGN))\n",
    "WHERE EXTRACT(YEAR FROM a.\"Date\") = {YEAR}\n",
    "  AND EXTRACT(MONTH FROM a.\"Date\") = {MONTH}\n",
    "\"\"\"\n",
    "\n",
    "# ========= RUN =========\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "if df.empty:\n",
    "    print(f\"No rows for {MONTH}/{YEAR}.\")\n",
    "else:\n",
    "    # --- Normalize column names to match DESIRED_HEADERS and create blanks as needed\n",
    "    rename_map = {\n",
    "        \"Site/Publisher\": \"Site / Publisher\",\n",
    "        \"Site/Publisher ID\": \"Site / Publisher ID\",\n",
    "        \"Click Through Rate\": \"Click through Rate\",  # align case/style\n",
    "        \"Engagements\": \"Engagement\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Ensure GMID exists as an empty column\n",
    "    if \"GMID\" not in df.columns:\n",
    "        df[\"GMID\"] = \"\"\n",
    "\n",
    "    # Ensure all desired headers exist (create empty if missing)\n",
    "    for col in DESIRED_HEADERS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    # --- Date parsing & labels\n",
    "    dt = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    if dt.isna().all():\n",
    "        raise ValueError(\"All 'Date' values are NaT after parsing; check source data.\")\n",
    "    df[\"__DATE_PARSED__\"] = dt\n",
    "\n",
    "    df[\"__MONTH_YEAR__\"] = df[\"__DATE_PARSED__\"].dt.strftime(\"%B_%Y\")\n",
    "    month_str = df[\"__DATE_PARSED__\"].dt.strftime(\"%B\").iloc[0]\n",
    "    year_int = int(df[\"__DATE_PARSED__\"].dt.year.iloc[0])\n",
    "    folder_month_label = f\"{month_str} {year_int}\"\n",
    "    df[\"__CHANNEL__\"] = \"Social\"\n",
    "\n",
    "    # --- Campaign type column resolution\n",
    "    if \"Campaign Type\" in df.columns:\n",
    "        COL_CTYPE = \"Campaign Type\"\n",
    "    elif \"Brand Type\" in df.columns:\n",
    "        COL_CTYPE = \"Brand Type\"\n",
    "    else:\n",
    "        COL_CTYPE = \"__CTYPE__\"\n",
    "        df[COL_CTYPE] = \"NA\"\n",
    "\n",
    "    # --- Strategy: prefer lookup, else derive\n",
    "    if \"LOOKUP_STRATEGY\" not in df.columns:\n",
    "        df[\"LOOKUP_STRATEGY\"] = \"\"\n",
    "    df[\"LOOKUP_STRATEGY\"] = df[\"LOOKUP_STRATEGY\"].fillna(\"\")\n",
    "\n",
    "    def _get_site_publisher(row):\n",
    "        # Be resilient to either header variant (we normalized to \"Site / Publisher\")\n",
    "        return row.get(\"Site / Publisher\", \"\")\n",
    "\n",
    "    df[\"__STRATEGY__\"] = df.apply(\n",
    "        lambda r: (r[\"LOOKUP_STRATEGY\"].strip()\n",
    "                   if isinstance(r[\"LOOKUP_STRATEGY\"], str) and r[\"LOOKUP_STRATEGY\"].strip()\n",
    "                   else derive_strategy_from_fields(_get_site_publisher(r), r.get(\"Campaign\", \"\"))),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # --- Ensure TA is never blank: TA -> Brand -> Franchise -> 'NA'\n",
    "    df[\"TA\"] = (\n",
    "        df.get(\"TA\", \"\")\n",
    "          .astype(str).str.strip()\n",
    "          .where(lambda s: s.ne(\"\"), df.get(\"Brand\", \"\"))\n",
    "          .where(lambda s: s.astype(str).str.strip().ne(\"\"), df.get(\"Franchise\", \"NA\"))\n",
    "    )\n",
    "\n",
    "    # --- Brand file column presence flag\n",
    "    brand_file_col = \"BRAND_FILE_NAME\" if \"BRAND_FILE_NAME\" in df.columns else None\n",
    "    has_brand_file = brand_file_col is not None\n",
    "\n",
    "    # --- Output folder (Social)\n",
    "    out_base = os.path.join(BASE_DIR, f\"{folder_month_label} Data Feed Delivery\", \"Social\")\n",
    "    os.makedirs(out_base, exist_ok=True)\n",
    "\n",
    "    # --- Group & write\n",
    "    group_cols = [\"TA\", \"__STRATEGY__\", \"__CHANNEL__\", COL_CTYPE, \"__MONTH_YEAR__\"]\n",
    "    if has_brand_file:\n",
    "        group_cols_with_brand = group_cols + [brand_file_col]\n",
    "    else:\n",
    "        df[\"__DUMMY__\"] = \"\"\n",
    "        group_cols_with_brand = group_cols + [\"__DUMMY__\"]\n",
    "\n",
    "    files_written = 0\n",
    "    rows_written = 0\n",
    "\n",
    "    for keys, g in df.groupby(group_cols_with_brand, dropna=False):\n",
    "        if has_brand_file:\n",
    "            ta, strat, channel, ctype_val, month_year, brand_file = keys\n",
    "        else:\n",
    "            ta, strat, channel, ctype_val, month_year, _ = keys\n",
    "            brand_file = \"\"\n",
    "\n",
    "        # Filename rules:\n",
    "        # - BRANDED:   TA_STRATEGY_CHANNEL_CTYPE_MONTHYEAR.csv\n",
    "        # - UNBRANDED: TA_STRATEGY_CHANNEL_CTYPE_BRANDFILE_MONTHYEAR.csv\n",
    "        # - Other:     TA_STRATEGY_CHANNEL_CTYPEorNA_MONTHYEAR.csv\n",
    "        ctype_tc = title_case_str(ctype_val)\n",
    "        ctype_upper = (ctype_tc or \"\").upper()\n",
    "        if ctype_upper == \"BRANDED\":\n",
    "            parts = [safe_token(ta), safe_token(strat), safe_token(channel), safe_token(ctype_tc), safe_token(month_year)]\n",
    "        elif ctype_upper == \"UNBRANDED\":\n",
    "            parts = [\n",
    "                safe_token(ta),\n",
    "                safe_token(strat),\n",
    "                safe_token(channel),\n",
    "                safe_token(ctype_tc),\n",
    "                safe_token(brand_file),\n",
    "                safe_token(month_year),\n",
    "            ]\n",
    "        else:\n",
    "            parts = [safe_token(ta), safe_token(strat), safe_token(channel), safe_token(ctype_tc or \"NA\"), safe_token(month_year)]\n",
    "\n",
    "        filename = \"_\".join([p for p in parts if p]) + \".csv\"\n",
    "        out_path = os.path.join(out_base, filename)\n",
    "\n",
    "        # Column selection in desired order (now guaranteed to exist)\n",
    "        present_headers = [c for c in DESIRED_HEADERS if c in g.columns and c in df.columns]\n",
    "        if not present_headers:\n",
    "            present_headers = list(g.columns)\n",
    "\n",
    "        # Stable sort by parsed date (keep mixed-case headers)\n",
    "        g_out = g[present_headers].copy()\n",
    "        g_out = g_out.assign(__DATE_PARSED__=g[\"__DATE_PARSED__\"]).sort_values(\n",
    "            by=\"__DATE_PARSED__\", na_position=\"last\"\n",
    "        ).drop(columns=\"__DATE_PARSED__\")\n",
    "\n",
    "        g_out.to_csv(out_path, index=False)\n",
    "        files_written += 1\n",
    "        rows_written += len(g_out)\n",
    "        print(f\"Wrote: {out_path}  (rows: {len(g_out)})\")\n",
    "\n",
    "    print(f\"\\nDone. Files: {files_written}  Total rows written: {rows_written}\")\n",
    "\n",
    "    # --- Optional: quick debug of why lookup missed (if any)\n",
    "    # This lists up to 50 distinct campaigns where LOOKUP_STRATEGY was blank and we derived it\n",
    "    derived_mask = df[\"LOOKUP_STRATEGY\"].astype(str).str.strip().eq(\"\")\n",
    "    if derived_mask.any():\n",
    "        print(\"\\n Campaigns missing strategy in lookup (showing up to 50):\")\n",
    "        print(df.loc[derived_mask, \"Campaign\"].astype(str).str.strip().drop_duplicates().head(50).to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d748902",
   "metadata": {},
   "source": [
    "# SUMMARY GENERATE\n",
    "\n",
    "## UPDATE HE TARGET MONTH FOR THE MONTH YOU ARE RUNNING DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DCM] rows: 249\n",
      "[DCM_AUD] rows: 7\n",
      "[SearchKey] rows: 43\n",
      "[SearchGeo] rows: 43\n",
      "[Social] rows: 110\n",
      "\n",
      " Wrote: C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\\May 2025 Data Validation Summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "\n",
    "TARGET_MONTH = 8          \n",
    "TARGET_YEAR  = 2025       \n",
    "OUTPUT_DIR   = r\"C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MONTH_NAME   = calendar.month_name[TARGET_MONTH]\n",
    "FILE_BASENAME = f\"{MONTH_NAME} {TARGET_YEAR} Data Validation Summary.xlsx\"\n",
    "OUTPUT_PATH   = os.path.join(OUTPUT_DIR, FILE_BASENAME)\n",
    "\n",
    "# ---- SQL templates (month is injected) ----\n",
    "SQL_QUERIES = {\n",
    "    \"DCM\": f\"\"\"\n",
    "        SELECT \n",
    "            \"Franchise\" AS \"Franchise\",\n",
    "            \"Brand\" AS \"Brand\",\n",
    "            \"Brand Indication/Campaign\" AS \"Brand Indication/Campaign\",\n",
    "            \"HCP/DTP\" AS \"Strategy\",\n",
    "            \"Campaign\",\n",
    "            \"Channel\" AS \"Channel\",\n",
    "            CASE WHEN \"Site/Publisher\" = 'MATTERKIND' THEN 'Addressable' ELSE 'Direct' END AS \"Addressable/Direct\",\n",
    "            \"Site/Publisher\" AS \"Site/Publisher\",\n",
    "            MONTHNAME(date) || '_' || TO_CHAR(date, 'YYYY') AS month_year,\n",
    "            SUM(\"Media Impressions\") AS \"Sum_Media Impressions\",\n",
    "            SUM(\"Media Clicks\") AS \"Sum_Media Clicks\",\n",
    "            SUM(\"Video Completes\") AS \"Sum_Video Completes\",\n",
    "            COUNT(*) AS \"Count\"\n",
    "        FROM DATAFEED_DCM\n",
    "        WHERE MONTH(date) = {TARGET_MONTH} AND YEAR(date) = {TARGET_YEAR}\n",
    "        GROUP BY ALL\n",
    "    \"\"\",\n",
    "\n",
    "    \"DCM_AUD\": f\"\"\"\n",
    "        SELECT \n",
    "            \"Franchise\" AS \"Franchise\",\n",
    "            \"Brand\" AS \"Brand\",\n",
    "            \"Brand Indication/Campaign\" AS \"Brand Indication/Campaign\",\n",
    "            \"HCP/DTP\" AS \"Strategy\",\n",
    "            \"Campaign\",\n",
    "            \"Channel\" AS \"Channel\",\n",
    "            CASE WHEN \"Site/Publisher\" = 'MATTERKIND' THEN 'Addressable' ELSE 'Direct' END AS \"Addressable/Direct\",\n",
    "            \"Site/Publisher\" AS \"Site/Publisher\",\n",
    "            MONTHNAME(date) || '_' || TO_CHAR(date, 'YYYY') AS month_year,\n",
    "            SUM(\"Media Impressions\") AS \"Sum_Media Impressions\",\n",
    "            SUM(\"Media Clicks\") AS \"Sum_Media Clicks\",\n",
    "            SUM(\"Audio Completes\") AS \"Sum_Audio Completes\",\n",
    "            COUNT(*) AS \"Count\"\n",
    "        FROM DATAFEED_DCM_AUDIO\n",
    "        WHERE \"Channel\" = 'Audio' AND MONTH(date) = {TARGET_MONTH} AND YEAR(date) = {TARGET_YEAR}\n",
    "        GROUP BY ALL\n",
    "    \"\"\",\n",
    "\n",
    "    \"SearchKey\": f\"\"\"\n",
    "        SELECT \n",
    "            \"Franchise\",\n",
    "            \"Brand\",\n",
    "            \"Brand Indication/Campaign\",\n",
    "            UPPER(SPLIT_PART(\"Campaign\", '_', 5))  AS \"Strategy\",\n",
    "            \"Engine\",\n",
    "            MONTHNAME(\"Date\") || '_' || TO_CHAR(\"Date\", 'YYYY') AS month_year,\n",
    "            SUM(\"Media Impressions\") AS \"Sum_Media Impressions\",\n",
    "            SUM(\"Media Clicks\") AS \"Sum_Media Clicks\",\n",
    "            SUM(\"Media Cost (USD)\") AS \"Sum_Media Cost (USD)\",\n",
    "            COUNT(*) AS \"Count\"\n",
    "        FROM DATAFEED_SEARCH_KEY\n",
    "        WHERE MONTH(\"Date\") = {TARGET_MONTH} AND YEAR(\"Date\") = {TARGET_YEAR}\n",
    "        GROUP BY ALL\n",
    "    \"\"\",\n",
    "\n",
    "    \"SearchGeo\": f\"\"\"\n",
    "        SELECT \n",
    "            \"Franchise\",\n",
    "            \"Brand\",\n",
    "            \"Brand Indication/Campaign\",\n",
    "            UPPER(SPLIT_PART(\"Campaign\", '_', 5))  AS \"Strategy\",\n",
    "            \"Engine\",\n",
    "            MONTHNAME(\"Date\") || '_' || TO_CHAR(\"Date\", 'YYYY') AS month_year,\n",
    "            SUM(\"Media Impressions\") AS \"Sum_Media Impressions\",\n",
    "            SUM(\"Media Clicks\") AS \"Sum_Media Clicks\",\n",
    "            SUM(\"Media Cost (USD)\") AS \"Sum_Media Cost (USD)\",\n",
    "            COUNT(*) AS \"Count\"\n",
    "        FROM DATAFEED_SEARCH_GEO\n",
    "        WHERE MONTH(\"Date\") = {TARGET_MONTH} AND YEAR(\"Date\") = {TARGET_YEAR}\n",
    "        GROUP BY ALL\n",
    "    \"\"\",\n",
    "\n",
    "    \"Social\": f\"\"\"\n",
    "        SELECT \n",
    "            \"Franchise\" AS \"Franchise\",\n",
    "            \"Brand\" AS \"Brand\",\n",
    "            \"Brand Indication/Campaign\" AS \"Brand Indication/Campaign\",\n",
    "            \"HCP/DTP\" AS \"Strategy\",\n",
    "            \"Campaign\",\n",
    "            \"Site/Publisher\" AS \"Site/Platform\",\n",
    "            SUM(\"Media Cost(USD)\") AS \"Sum_Media Cost (USD)\",\n",
    "            SUM(\"Media Impressions\") AS \"Sum_Media Impressions\",\n",
    "            SUM(\"Media Clicks\") AS \"Sum_Media Clicks\",\n",
    "            SUM(\"Video Completes\") AS \"Sum_Video Completes\",\n",
    "            SUM(\"Video Views\") AS \"Sum_Video Views\",\n",
    "            SUM(\"Engagements\") AS \"Sum_Engagement\",\n",
    "            SUM(\"Click Through Rate\") AS \"Click Through Rate\",\n",
    "            COUNT(*) AS \"Count\"\n",
    "        FROM DATAFEED_SOCIAL\n",
    "        WHERE MONTH(\"Date\") = {TARGET_MONTH} AND YEAR(\"Date\") = {TARGET_YEAR}\n",
    "        GROUP BY ALL\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Explicit sheet order in the final workbook\n",
    "SHEET_ORDER = [\"DCM\", \"DCM_AUD\", \"SearchKey\", \"SearchGeo\", \"Social\"]\n",
    "\n",
    "def fetch_df(conn, sql: str) -> pd.DataFrame:\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        cols = [c[0] for c in cur.description]\n",
    "        rows = cur.fetchall()\n",
    "        return pd.DataFrame(rows, columns=cols)\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "def run_and_export_one_workbook(conn):\n",
    "    # Create workbook with one sheet per query (in specified order)\n",
    "    with pd.ExcelWriter(OUTPUT_PATH, engine=\"openpyxl\") as xw:\n",
    "        for name in SHEET_ORDER:\n",
    "            sql = SQL_QUERIES[name]\n",
    "            df = fetch_df(conn, sql)\n",
    "            df.to_excel(xw, index=False, sheet_name=name)\n",
    "            print(f\"[{name}] rows: {len(df):,}\")\n",
    "\n",
    "    print(f\"\\n Wrote: {OUTPUT_PATH}\")\n",
    "\n",
    "# ---- Run it ----\n",
    "run_and_export_one_workbook(conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda3a61",
   "metadata": {},
   "source": [
    "# FILE GENERATION FOR MONTHLY IW\n",
    "## CHANGE MONTH LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ac661",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_LABEL = \"August 2025\"  \n",
    "BASE_OUT = Path(r\"C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\")\n",
    "\n",
    "# Existing monthly delivery folder name found under each source tree\n",
    "SOURCE_DELIVERY_FOLDER = f\"{MONTH_LABEL} Data Feed Delivery\"\n",
    "\n",
    "# New folder were creating\n",
    "IW_DELIVERY_FOLDER = f\"IW {MONTH_LABEL} Data Feed Delivery\"\n",
    "\n",
    "# Channel subfolders to create (matches your layout)\n",
    "CHANNEL_FOLDERS = [\n",
    "    \"AUD\", \"CTV\", \"Custom\", \"Display\", \"FEP\", \"Newsletter\", \"Non-DMA AUD\",\n",
    "    \"OLV\", \"SearchGeo\", \"SearchKey\", \"Social\"\n",
    "]\n",
    "\n",
    "# Copy toggles\n",
    "COPY_SOCIAL_AND_SEARCH = True\n",
    "COPY_DCM = True\n",
    "COPY_NON_DMA_AUD = True\n",
    "\n",
    "# File types to copy (None = all)\n",
    "EXT_WHITELIST = None  # e.g., {\".csv\", \".xlsx\", \".txt\"}\n",
    "\n",
    "# Overwrite behavior\n",
    "OVERWRITE = False  # False -> add \" (1)\" / \" (2)\" if name collides\n",
    "\n",
    "# Source roots\n",
    "SOURCE_ROOTS = {\n",
    "    \"Social\": BASE_OUT / \"Social\",\n",
    "    \"SEARCH GEO\": BASE_OUT / \"SEARCH GEO\",\n",
    "    \"SEARCH KEY\": BASE_OUT / \"SEARCH KEY\",\n",
    "    \"DCM\": BASE_OUT / \"DCM\",\n",
    "    \"NON-DMA AUD\": BASE_OUT / \"Non-DMA AUD\", \n",
    "}\n",
    "\n",
    "# Some sources have an extra nested folder inside the monthly delivery folder\n",
    "NESTED_INSIDE_DELIVERY = {\n",
    "    \"Social\": \"Social\",\n",
    "    \"SEARCH GEO\": \"SearchGeo\",\n",
    "    \"SEARCH KEY\": \"SearchKey\",\n",
    "    # DCM and Non-DMA AUD usually flat; leave out unless yours has a nested folder\n",
    "}\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def should_copy(path: Path) -> bool:\n",
    "    if not path.is_file():\n",
    "        return False\n",
    "    if path.name.startswith(\"~$\"):  # skip temp Office files\n",
    "        return False\n",
    "    if EXT_WHITELIST is None:\n",
    "        return True\n",
    "    return path.suffix.lower() in EXT_WHITELIST\n",
    "\n",
    "\n",
    "def unique_destination(dest: Path) -> Path:\n",
    "    if OVERWRITE or not dest.exists():\n",
    "        return dest\n",
    "    stem, suffix = dest.stem, dest.suffix\n",
    "    i = 1\n",
    "    while True:\n",
    "        candidate = dest.with_name(f\"{stem} ({i}){suffix}\")\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def find_delivery_dirs(root: Path, target_name: str):\n",
    "    \"\"\"Yield every directory named exactly target_name anywhere under root (case-insensitive).\"\"\"\n",
    "    target_low = target_name.strip().lower()\n",
    "    for dirpath, dirnames, _ in os.walk(root):\n",
    "        if os.path.basename(dirpath).strip().lower() == target_low:\n",
    "            yield Path(dirpath)\n",
    "\n",
    "\n",
    "def copy_tree(src_dir: Path, dst_dir: Path) -> int:\n",
    "    \"\"\"Copy all files (respecting EXT_WHITELIST/OVERWRITE) from src_dir (recursive) to dst_dir.\"\"\"\n",
    "    if not src_dir.exists():\n",
    "        return 0\n",
    "    count = 0\n",
    "    for item in src_dir.rglob(\"*\"):\n",
    "        if not should_copy(item):\n",
    "            continue\n",
    "        rel = item.relative_to(src_dir)\n",
    "        out_path = dst_dir / rel\n",
    "        ensure_dir(out_path.parent)\n",
    "        out_path = unique_destination(out_path)\n",
    "        shutil.copy2(item, out_path)\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# -------- DCM channel detection (filename-based) --------\n",
    "# Maps filename keywords to IW channel folder names.\n",
    "# Example: any file whose name has whole-word \"audio\" routes to \"AUD\".\n",
    "DCM_CHANNEL_REGEX = re.compile(\n",
    "    r\"\\b(audio|display|ctv|custom|fep|newsletter|olv)\\b\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "DCM_KEYWORD_TO_FOLDER = {\n",
    "    \"audio\": \"AUD\",          # your note: AUD is Audio\n",
    "    \"display\": \"Display\",\n",
    "    \"ctv\": \"CTV\",\n",
    "    \"custom\": \"Custom\",\n",
    "    \"fep\": \"FEP\",\n",
    "    \"newsletter\": \"Newsletter\",\n",
    "    \"olv\": \"OLV\",\n",
    "}\n",
    "\n",
    "def detect_dcm_channel(filename: str) -> str | None:\n",
    "    \"\"\"Return IW channel folder name for a DCM file, or None if no match.\"\"\"\n",
    "    name = Path(filename).stem  # ignore extension for matching\n",
    "    m = DCM_CHANNEL_REGEX.search(name.replace(\"_\", \" \"))\n",
    "    if not m:\n",
    "        return None\n",
    "    keyword = m.group(1).lower()\n",
    "    return DCM_KEYWORD_TO_FOLDER.get(keyword)\n",
    "\n",
    "\n",
    "# -------------------- Main --------------------\n",
    "def main():\n",
    "    iw_root = ensure_dir(BASE_OUT / IW_DELIVERY_FOLDER)\n",
    "    for ch in CHANNEL_FOLDERS:\n",
    "        ensure_dir(iw_root / ch)\n",
    "\n",
    "    print(f\" Created: {iw_root}\")\n",
    "    print(\" Subfolders:\")\n",
    "    for ch in CHANNEL_FOLDERS:\n",
    "        print(f\"  - {iw_root / ch}\")\n",
    "\n",
    "    # Copy Social / Search\n",
    "    if COPY_SOCIAL_AND_SEARCH:\n",
    "        for key in (\"Social\", \"SEARCH GEO\", \"SEARCH KEY\"):\n",
    "            src_root = SOURCE_ROOTS[key]\n",
    "            found_any = False\n",
    "            total = 0\n",
    "            for delivery_dir in find_delivery_dirs(src_root, SOURCE_DELIVERY_FOLDER):\n",
    "                found_any = True\n",
    "                nested = NESTED_INSIDE_DELIVERY.get(key)\n",
    "                search_dir = delivery_dir / nested if nested else delivery_dir\n",
    "                if nested and not search_dir.exists():\n",
    "                    search_dir = delivery_dir  # fallback\n",
    "\n",
    "                if key == \"Social\":\n",
    "                    dest = iw_root / \"Social\"\n",
    "                elif key == \"SEARCH GEO\":\n",
    "                    dest = iw_root / \"SearchGeo\"\n",
    "                else:\n",
    "                    dest = iw_root / \"SearchKey\"\n",
    "\n",
    "                copied = copy_tree(search_dir, dest)\n",
    "                total += copied\n",
    "                print(f\" Copied {copied} files from {search_dir}  {dest}\")\n",
    "            if not found_any:\n",
    "                print(f\" No '{SOURCE_DELIVERY_FOLDER}' found under {src_root}\")\n",
    "            else:\n",
    "                print(f\" {key}: total files copied = {total}\")\n",
    "\n",
    "    # Copy DCM with channel routing\n",
    "    if COPY_DCM:\n",
    "        src_root = SOURCE_ROOTS[\"DCM\"]\n",
    "        found_any = False\n",
    "        total = 0\n",
    "        routed = 0\n",
    "        skipped = []\n",
    "        for delivery_dir in find_delivery_dirs(src_root, SOURCE_DELIVERY_FOLDER):\n",
    "            found_any = True\n",
    "            search_dir = delivery_dir  # typically flat\n",
    "            for item in search_dir.rglob(\"*\"):\n",
    "                if not should_copy(item):\n",
    "                    continue\n",
    "                channel_folder = detect_dcm_channel(item.name)\n",
    "                if not channel_folder:\n",
    "                    skipped.append(str(item))\n",
    "                    continue\n",
    "                dest_dir = iw_root / channel_folder\n",
    "                ensure_dir(dest_dir)\n",
    "                out_path = unique_destination(dest_dir / item.name)\n",
    "                shutil.copy2(item, out_path)\n",
    "                total += 1\n",
    "                routed += 1\n",
    "        if not found_any:\n",
    "            print(f\" No '{SOURCE_DELIVERY_FOLDER}' found under {src_root}\")\n",
    "        print(f\" DCM: files copied = {total}, routed = {routed}\")\n",
    "        if skipped:\n",
    "            print(\" DCM files skipped (no channel keyword match):\")\n",
    "            for s in skipped:\n",
    "                print(f\"  - {s}\")\n",
    "\n",
    "    # Copy Non-DMA AUD\n",
    "    if COPY_NON_DMA_AUD:\n",
    "        src_root = SOURCE_ROOTS[\"NON-DMA AUD\"]\n",
    "        found_any = False\n",
    "        total = 0\n",
    "        for delivery_dir in find_delivery_dirs(src_root, SOURCE_DELIVERY_FOLDER):\n",
    "            found_any = True\n",
    "            search_dir = delivery_dir  # assume flat; customize if you actually nest\n",
    "            dest = iw_root / \"Non-DMA AUD\"\n",
    "            copied = copy_tree(search_dir, dest)\n",
    "            total += copied\n",
    "            print(f\" Copied {copied} files from {search_dir}  {dest}\")\n",
    "        if not found_any:\n",
    "            print(f\" No '{SOURCE_DELIVERY_FOLDER}' found under {src_root}\")\n",
    "        else:\n",
    "            print(f\" Non-DMA AUD: total files copied = {total}\")\n",
    "\n",
    "    print(\"\\n Done. All copies complete. (No files moved.)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit(\"\\nInterrupted by user.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56d68f",
   "metadata": {},
   "source": [
    "# MOVE FILES FOR IPSOS\n",
    "\n",
    "### UPDATE THE MONTH LABEL AND DELIVERY FOLDER NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_LABEL = \"August 2025\"            \n",
    "DELIVERY_FOLDER = \"August 2025 Data Feed Delivery\"  \n",
    "BASE_OUT = Path(r\"C:\\Users\\tanzil.baraskar\\OneDrive - Interpublic\\Desktop\\Data Feed File Downloads\")\n",
    "\n",
    "SOURCE_ROOTS = [\n",
    "    BASE_OUT / \"Social\",\n",
    "    BASE_OUT / \"DCM\",\n",
    "    BASE_OUT / \"SEARCH GEO\",\n",
    "    BASE_OUT / \"SEARCH KEY\",\n",
    "]\n",
    "\n",
    "DEST_ROOT = BASE_OUT / f\"IPSOS {MONTH_LABEL} Data Feeds\"\n",
    "\n",
    "BRAND_FOLDERS = [\"Biktarvy\", \"Epclusa\", \"Descovy\", \"Trodelvy\", \"Yeztugo\"]\n",
    "\n",
    "OVERWRITE = False       # If True, overwrite existing files. If False, auto-rename with (1), (2)...\n",
    "DRY_RUN = False         # If True, print what would happen without copying\n",
    "\n",
    "\n",
    "ROUTING_RULES = [\n",
    "    ((\"HIV_Treatment\",),                     \"Biktarvy\"),\n",
    "    ((\"ONC_Trodelvy\", \"ONCOLOGY_Trodelvy\"),  \"Trodelvy\"),\n",
    "    ((\"HCV\",),                               \"Epclusa\"),\n",
    "    ((\"HIV_PrEP_LEN\",),                      \"Yeztugo\"),\n",
    "    ((\"HIV_PrEP\",),                          \"Descovy\"),\n",
    "]\n",
    "\n",
    "\n",
    "EXT_WHITELIST = None\n",
    "\n",
    "\n",
    "NESTED_DELIVERY_SUBFOLDER = {\n",
    "    \"SEARCH KEY\": \"SearchKey\",\n",
    "    \"SEARCH GEO\": \"SearchGeo\",\n",
    "    \"SOCIAL\": \"Social\",\n",
    "}\n",
    "\n",
    "def ensure_dirs():\n",
    "    DEST_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    for b in BRAND_FOLDERS:\n",
    "        (DEST_ROOT / b).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Case-insensitive, strip leading/trailing spaces for safer matching.\"\"\"\n",
    "    return s.strip().lower()\n",
    "\n",
    "\n",
    "def choose_brand(filename: str) -> str | None:\n",
    "    \"\"\"Return brand folder name based on filename prefix rules; None if no match.\"\"\"\n",
    "    name = Path(filename).name  # just the file name\n",
    "    low = normalize(name)\n",
    "    for prefixes, brand in ROUTING_RULES:\n",
    "        for p in prefixes:\n",
    "            if low.startswith(normalize(p)):\n",
    "                return brand\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_delivery_dirs(root: Path, target_dirname: str):\n",
    "    \"\"\"Yield every directory named exactly target_dirname anywhere under root.\"\"\"\n",
    "    target_low = normalize(target_dirname)\n",
    "    for dirpath, dirnames, _filenames in os.walk(root):\n",
    "        # Check if current dir's name matches DELIVERY_FOLDER (case-insensitive)\n",
    "        if normalize(os.path.basename(dirpath)) == target_low:\n",
    "            yield Path(dirpath)\n",
    "\n",
    "\n",
    "def unique_destination(dest: Path) -> Path:\n",
    "    \"\"\"If OVERWRITE=False and file exists, append ' (n)' before suffix to avoid clobbering.\"\"\"\n",
    "    if OVERWRITE or not dest.exists():\n",
    "        return dest\n",
    "    stem, suffix = dest.stem, dest.suffix\n",
    "    i = 1\n",
    "    while True:\n",
    "        candidate = dest.with_name(f\"{stem} ({i}){suffix}\")\n",
    "        if not candidate.exists():\n",
    "            return candidate\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def should_copy_file(p: Path) -> bool:\n",
    "    if not p.is_file():\n",
    "        return False\n",
    "    if p.name.startswith(\"~$\"):  # skip temp Office files\n",
    "        return False\n",
    "    if EXT_WHITELIST is None:\n",
    "        return True\n",
    "    return p.suffix.lower() in EXT_WHITELIST\n",
    "\n",
    "\n",
    "# ========================== Main ===================================\n",
    "def main():\n",
    "    ensure_dirs()\n",
    "\n",
    "    summary = defaultdict(int)\n",
    "    skipped_no_brand = []\n",
    "    scanned_delivery_dirs = []\n",
    "\n",
    "    for src_root in SOURCE_ROOTS:\n",
    "        if not src_root.exists():\n",
    "            print(f\" Source root not found: {src_root}\")\n",
    "            continue\n",
    "\n",
    "        src_key = src_root.name.upper()\n",
    "        expected_nested = NESTED_DELIVERY_SUBFOLDER.get(src_key, None)\n",
    "\n",
    "        for delivery_dir in find_delivery_dirs(src_root, DELIVERY_FOLDER):\n",
    "            scanned_delivery_dirs.append(delivery_dir)\n",
    "\n",
    "            # If this source expects a nested subfolder inside the delivery folder, use it.\n",
    "            search_dir = delivery_dir\n",
    "            if expected_nested:\n",
    "                candidate = delivery_dir / expected_nested\n",
    "                if candidate.exists() and candidate.is_dir():\n",
    "                    search_dir = candidate\n",
    "                else:\n",
    "                    print(f\" Expected nested folder missing for {src_root.name}: {candidate}\")\n",
    "\n",
    "            for item in search_dir.rglob(\"*\"):\n",
    "                if not should_copy_file(item):\n",
    "                    continue\n",
    "\n",
    "                brand = choose_brand(item.name)\n",
    "                if brand is None:\n",
    "                    skipped_no_brand.append(str(item))\n",
    "                    continue\n",
    "\n",
    "                dest_dir = DEST_ROOT / brand\n",
    "                dest_path = dest_dir / item.name\n",
    "                dest_path = unique_destination(dest_path)\n",
    "\n",
    "                if DRY_RUN:\n",
    "                    print(f\"[DRY RUN] COPY: {item} -> {dest_path}\")\n",
    "                else:\n",
    "                    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    shutil.copy2(item, dest_path)\n",
    "\n",
    "                summary[brand] += 1\n",
    "\n",
    "    # --------- Reporting ----------\n",
    "    print(\"\\n========== SUMMARY ==========\")\n",
    "    print(f\"Scanned delivery folders matching '{DELIVERY_FOLDER}':\")\n",
    "    if scanned_delivery_dirs:\n",
    "        for d in scanned_delivery_dirs:\n",
    "            print(f\"  - {d}\")\n",
    "    else:\n",
    "        print(\"  (none found)\")\n",
    "\n",
    "    print(\"\\nCopied files by brand:\")\n",
    "    for b in BRAND_FOLDERS:\n",
    "        print(f\"  {b}: {summary[b]}\")\n",
    "\n",
    "    if skipped_no_brand:\n",
    "        print(\"\\nSkipped (no routing rule matched):\")\n",
    "        for s in skipped_no_brand:\n",
    "            print(f\"  - {s}\")\n",
    "\n",
    "    if DRY_RUN:\n",
    "        print(\"\\nNote: DRY_RUN=True, no files were actually copied.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit(\"\\nInterrupted by user.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
